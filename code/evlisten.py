import os
import sys
from twisted.web import server, resource, http
from twisted.internet import reactor, endpoints
import json
import logging
import requests
import urlparse
import math
import time
import datetime 
import tinydb 
import urllib2, os, sys, subprocess
import googleapiclient.discovery
import paramiko


""" 
Usage: http://localhost:7878/?preempted=abacus
http://localhost:7878/?finished=jobid 
http://localhost:7878/?launch_cluster=True&namegrp=abra&num_nodes=4&mtype=n1-highcpu-16&start_id=1&slurm_master=ubslurm1

""" 


##################################################    ##################################################

class evlisten(resource.Resource):
    isLeaf = True  # Required by twisted?
    project='first-220321'
    zone='us-east1-b'
    namegrp="abra"

    mtypes_highcpu=['n1-highcpu-16','n1-highcpu-2','n1-highcpu-32','n1-highcpu-4','n1-highcpu-64','n1-highcpu-8']
    mtypes_stan=['n1-standard-1','n1-standard-16','n1-standard-2','n1-standard-32','n1-standard-4','n1-standard-64','n1-standard-8']
    zones=['us-central1-f','us-east1-b','us-east4-c','europe-west4-a','europe-north1-a','asia-southeast1-b']
    
    ##################################################

    def __init__(self):
        #Open the job and vmdb json databases?
        self.compute = googleapiclient.discovery.build('compute', 'v1')
    
    ##################################################
    
    def get_jobdb(self):
        return tinydb.TinyDB('jobdb.json')

    def get_vmdb(self):
        return tinydb.TinyDB('vmdb.json')

    ##################################################


    def machine_type(self, m):
        #Cat proc/cpuinfo, and some memory ? 
        #cpus = num_cpus()
        cpus = int(m.split('-')[-1])
        machine = {'sockets': 1, 'cores': cpus/2, 'threads': 2, 'memory': 1000}
        return machine 

    def start_instance(self, mtype, zone, name, startupscriptstr):
        """ Hidden inputs: VM-image """

        machine_type = "zones/{}/machineTypes/{}".format(zone, mtype)

        instance_body={
            'name':name,
            'machineType':machine_type, 
            'scheduling':
              {
            'preemptible': 'true'
              },
            'disks': [
            {
                'boot': True,
                'autoDelete': True,
                'initializeParams': {
                    'sourceImage': "global/images/ubs2"
                    }
            }],
            'metadata' : {
                "items" : [
                    {
                        "key": "startup-script",
                        "value":startupscriptstr
                    }
                ]
            },
            'networkInterfaces': [{
            'network': 'global/networks/default',
            'accessConfigs': [
                {'type': 'ONE_TO_ONE_NAT', 'name': 'External NAT'}
            ]
        }],
        }
        print(str(instance_body))

        response = self.compute.instances().insert(project=self.project,zone=self.zone,body=instance_body).execute()
        return response 

    ##################################################

    def generate_slurm_config(self, slurm_master, machine, compute_nodes):
        slurmconfstr = """
# slurm.conf file generated by configurator easy.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
ControlMachine={slurm_master}
AuthType=auth/none
#CheckpointType=checkpoint/none
#CryptoType=crypto/munge
#ControlAddr=
#
#MailProg=/bin/mail
MpiDefault=none
#MpiParams=ports=#-#
ProctrackType=proctrack/pgid
ReturnToService=1
SlurmctldPidFile=/var/run/slurm-llnl/slurmctld.pid
#SlurmctldPort=6817
SlurmdPidFile=/var/run/slurm-llnl/slurmd.pid
#SlurmdPort=6818
SlurmdSpoolDir=/var/lib/slurm-llnl/slurmd
#SlurmUser=slurm
SlurmdUser=root
StateSaveLocation=/var/lib/slurm-llnl/slurmctld
SwitchType=switch/none
TaskPlugin=task/none
JobCredentialPrivateKey=/home/prateek3_14/slurmkey
#
JobCompType=jobcomp/filetxt
JobCompLoc=/var/log/slurmjobs
DebugFlags=NO_CONF_HASH
# TIMERS
#KillWait=30
#MinJobAge=300
#SlurmctldTimeout=120
#SlurmdTimeout=300
#
#
# SCHEDULING
FastSchedule=1
SchedulerType=sched/builtin
#SchedulerPort=7321
SelectType=select/linear
#
#
# LOGGING AND ACCOUNTING
AccountingStorageType=accounting_storage/none
ClusterName=ubslurm1
#JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/none
#SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log
#SlurmdDebug=3
SlurmdLogFile=/var/log/slurm-llnl/slurmd.log
SuspendTime=86400
#
#
# COMPUTE NODES \n""".\
        format(slurm_master=slurm_master)

        slurmconfstr += ' '.join(("NodeName=DEFAULT",
                              "Sockets="        + str(machine['sockets']),
                              "CoresPerSocket=" + str(machine['cores']),
                              "ThreadsPerCore=" + str(machine['threads']),
                              "State=UNKNOWN"))
        #state=CLOUD otherwise, but doesnt work? 
        
        slurmconfstr += '\n'
        #slurmconfstr += 'NodeName={slurm_master} \n'.format(slurm_master=slurm_master)
        slurmconfstr += 'NodeName={compute_nodes} \n'.format(compute_nodes=compute_nodes)


        slurmconfstr += "\n PartitionName=long Nodes={compute_nodes} Default=YES MaxTime=INFINITE State=UP \n".format(compute_nodes=compute_nodes)
        return slurmconfstr

    ##################################################

    def get_startup_script(self, slurmconfstr):
        """ Copy slurm config, and start slurmd. TODO: if master, then slurmctld"""

        startupscriptstr = """
#!/bin/bash

logger "Startup Script Begins.... "
logger "Running as `whoami`" 

systemctl stop slurmd 
systemctl stop slurmctld 

echo > /etc/slurm-llnl/slurm.conf

cat <<\EOF >> /etc/slurm-llnl/slurm.conf
{slurmconfstr}
EOF

systemctl start slurmd 

logger "Slurm conf applied, startup script ending" 

exit 0

    """.format(slurmconfstr=slurmconfstr)
        return startupscriptstr

    ##################################################

    def get_instance_ip(self, instance):
        #Let project and zone be global!
        return self.compute.instances().get(project=self.project, zone=self.zone, instance=instance).execute().get('networkInterfaces')[0].get('accessConfigs')[0].get('natIP')


    def gcp_ssh(self, instance):
        instance_ip = self.get_instance_ip(instance)
        #
        client=paramiko.SSHClient()
        client.load_system_host_keys()
        client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        #Copy the new slurm config file 
        client.connect(instance_ip, username='prateek3_14',key_filename='/home/prateeks/.ssh/gce')
        return client

    ##################################################

    def reconfig_master(self, master, slurmconf_str):
    
        client = self.gcp_ssh(master)

        i,o,e=client.exec_command("cat > /tmp/slurmconf")
        i.write(slurmconf_str)
        i.close()
        client.close() 
        #TODO: Can we reuse the same connection? 

        client = self.gcp_ssh(master)
        i,o,e=client.exec_command("sudo slurm_reload_cfg /tmp/slurmconf")
        client.close()
        #Above script copies new cfg and restarts slurm daemons 
        print("Master reconfigured")
        #TODO: Set up strigger node down for each launched node? 
        return 


    ##################################################

    def launch_cluster(self, namegrp, num_nodes, mtype, start_id=1, slurm_master=None):
        machine = self.machine_type(mtype)

        compute_nodes = namegrp+"[1-"+str(num_nodes*3)+"]"
        #*3 is just a margin for safety in case we fail and have to rerun the job without reconfiguring master 
        configstr = self.generate_slurm_config(slurm_master, machine, compute_nodes)
        cnodes_launched = []
        if slurm_master is None:
            print("Master must be running, not supported yet, exiting")
            return

        self.reconfig_master(slurm_master, configstr)

        for i in range(start_id, num_nodes+start_id): 
            name = namegrp+str(i)

            self.start_instance(mtype, self.zone, name, self.get_startup_script(configstr))
            cnodes_launched.append(name)
            #Add to the vmdb 
            vmdb = self.get_vmdb()
            t_start = datetime.datetime.now().isoformat()
            vmdb.insert({'vmname':name, 't_start':t_start, 'type':mtype, 'status':'running'})
            vmdb.close()

            time.sleep(5)
        print("Cluster Launched")
        return (slurm_master, cnodes_launched)

    ##################################################
    
    def run_job(self, master, num_nodes, cores):
        runfile = "/home/prateek3_14/sb_confinement.sh"
        simparams = ""
        sbatcmd = "sbatch --parsable -N {num_nodes} -c {cores} -n {num_nodes} {runfile} {simparams}".format(\
            num_nodes=num_nodes, cores=cores, runfile=runfile, simparams=simparams)
        #Just returns the job-id 

        sshclient = self.gcp_ssh(master)
        i,o,e = sshclient.exec_command(sbatcmd)
        jobid = o.read() 
        o.close()
        sshclient.close() 

        #TODO strigger
        strigger_fin_cmd = "strigger --set --fini --program /scispot/handle_fin.sh --jobid {}".format(jobid)
        strigger_fail_cmd = "strigger --set --jobid={} --down --program=/scispot/handle_fail.sh".format(jobid)

        # sshclient = self.gcp_ssh(master)
        # i,o,e = sshclient.exec_command(strigger_fin_cmd)
        # i,o,e = sshclient.exec_command(strigger_fail_cmd)
        # sshclient.close()
        
        #TODO: Update state of the job and add to job dict with the time we launched 
        #Metadata for a job: jobid, launchtime, sbatch config, simparams, codepath,.. 
        #starttime = time.get()
        jobdb = self.get_jobdb() 
        t_start = datetime.datetime.now().isoformat()
        jobdb.insert({'jobname':jobid, 't_start':t_start, 'resources':(num_nodes, cores), 'state':'running'})
        jobdb.close()
        return jobid

    
    ##################################################
    ################## Event Listeners ###############
    ##################################################
    
    
    def handle_finished(self, jobids):
        if jobids is None or len(jobids) is 0:
            print("empty job string, nothing to do")
            return
        fin_time = datetime.datetime.now().isoformat()
        jobdb = self.get_jobdb()
        
        for job in jobids:
            #Will be a string, but that is OK if the keys are strings?
            q = tinydb.Query()
            res = jobdb.search(q.jobname == job)
            if len(res) is 0:
                print("No matching job")
                jobdb.insert({'jobid':job, 't_finish':fin_time})
            else:
                jobdb.update({'t_finish':fin_time, 'state':'finished'}, q.jobname == job)
                
        jobdb.close() #For mutual exclusion

        self.get_next_job(jobids[0], "finished") 
        return
    
    ##################################################
    
    def get_next_job(self, jobid, state="finished"):
        """ Given a job id that has finished/failed, decide what to do """
        if state is "finished":
            #Generate the next job parameters
            pass
        elif state is "failed":
            #Need to decide whether to restart or ignore
            pass 


    ##################################################
    
    def handle_preempted(self, vmnames):
        """ We only get the jobid. Find vmids later by scanning compute instances list """ 
        if jobids is None or len(jobids) is 0:
            print("empty job string, nothing to do")
            return
        fin_time = datetime.datetime.now().isoformat()
        jobdb = self.get_jobdb()
        
        for job in jobids:
            #Will be a string, but that is OK if the keys are strings?
            q = tinydb.Query()
            res = jobdb.search(q.jobname == job)
            if len(res) is 0:
                print("No matching job")
                jobdb.insert({'jobid':job, 't_finish':fin_time})
            else:
                jobdb.update({'t_finish':fin_time, 'state':'failed'}, q.jobname == job)
                
        jobdb.close() #For mutual exclusion

        #TODO: We must spawn extra VMs first!!! Find out how many VMs are required?
        self.preemption_reaction(jobids[0])
        
        return 
    

    def preemption_reaction(self, jobid):
        #Replenish, restart, ignore, etc.
        self.replenish()
        
        #Decide if we want to rerun or not, or whatever 
        self.get_next_job(jobid, "failed") 


    
    ##################################################
    
    def parse_handle_req(self, req_args):
        if 'preempted' in req_args.keys():
            print("preempted")
            #Get the node id
            vals = req_args['preempted']
            self.handle_preempted(vals)
            
        elif 'finished' in req_args.keys():
            #Get the job id
            print("finished")
            vals = req_args['finished']
            self.handle_finished(vals)

        elif 'launch_cluster' in req_args.keys():
            print(req_args)
            self.launch_cluster(req_args['namegrp'][0], int(req_args['num_nodes'][0]), \
                                req_args['mtype'][0], int(req_args['start_id'][0]), \
                                req_args['slurm_master'][0])
        elif 'run_job' in req_args.keys():
            self.run_job(req_args['master'][0], req_args['num_nodes'][0],\
                         req_args['cores'][0])
            
        else:
            print("not understood")
            
        
        return "OK"

    ##################################################
                
    def render_GET(self, request):
            #    self.numberRequests += 1
        a = request.args
        print(a)
        to_ret = self.parse_handle_req(a)
        request.setHeader(b"content-type", b"text/plain")
        content = to_ret
        return content.encode("ascii")

##################################################

endpoints.serverFromString(reactor, "tcp:7878").listen(
    server.Site(evlisten()))

reactor.run()
