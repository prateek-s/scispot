{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2, os, sys, subprocess\n",
    "import googleapiclient.discovery\n",
    "import os, sys\n",
    "import random\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_instance_properties():\n",
    "    #compute-nodes: list of other instances that belong to cluster. Try using series-[1-100] notation\n",
    "    #name of slurm-master \n",
    "    #machine type, can we get that from /proc/cpuinfo? \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_cpus():\n",
    "    try:\n",
    "        np=subprocess.check_output('cat /proc/cpuinfo | grep processor | wc -l', shell=True)\n",
    "        p = int(np)\n",
    "        return p\n",
    "    except:\n",
    "        p = 1 \n",
    "        return p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def machine_type():\n",
    "    #Cat proc/cpuinfo, and some memory ? \n",
    "    cpus = num_cpus()\n",
    "    machine = {'sockets': 1, 'cores': cpus/2, 'threads': 2, 'memory': 1000}\n",
    "    return machine \n",
    "#TODO: Convert to string type so that we can use it? \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_master=\"ubslurm1\"\n",
    "compute_nodes=\"bravo[1-10]\"\n",
    "mc_string = \"CPUs=1 sockets=1 cores=1 threads=1\" \n",
    "machine=machine_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine={'sockets': 1, 'cores': 1, 'threads': 2, 'memory': 1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurmconfstr = \"\"\"\n",
    "# slurm.conf file generated by configurator easy.html.\n",
    "# Put this file on all nodes of your cluster.\n",
    "# See the slurm.conf man page for more information.\n",
    "#\n",
    "ControlMachine={slurm_master}\n",
    "AuthType=auth/none\n",
    "#CheckpointType=checkpoint/none\n",
    "#CryptoType=crypto/munge\n",
    "#ControlAddr=\n",
    "#\n",
    "#MailProg=/bin/mail\n",
    "MpiDefault=none\n",
    "#MpiParams=ports=#-#\n",
    "ProctrackType=proctrack/pgid\n",
    "ReturnToService=1\n",
    "SlurmctldPidFile=/var/run/slurm-llnl/slurmctld.pid\n",
    "#SlurmctldPort=6817\n",
    "SlurmdPidFile=/var/run/slurm-llnl/slurmd.pid\n",
    "#SlurmdPort=6818\n",
    "SlurmdSpoolDir=/var/lib/slurm-llnl/slurmd\n",
    "SlurmUser=slurm\n",
    "#SlurmdUser=root\n",
    "StateSaveLocation=/var/lib/slurm-llnl/slurmctld\n",
    "SwitchType=switch/none\n",
    "TaskPlugin=task/none\n",
    "JobCredentialPrivateKey=/home/prateek3_14/slurmkey\n",
    "#\n",
    "#\n",
    "# TIMERS\n",
    "#KillWait=30\n",
    "#MinJobAge=300\n",
    "#SlurmctldTimeout=120\n",
    "#SlurmdTimeout=300\n",
    "#\n",
    "#\n",
    "# SCHEDULING\n",
    "FastSchedule=1\n",
    "SchedulerType=sched/builtin\n",
    "#SchedulerPort=7321\n",
    "SelectType=select/linear\n",
    "#\n",
    "#\n",
    "# LOGGING AND ACCOUNTING\n",
    "AccountingStorageType=accounting_storage/none\n",
    "ClusterName=ubslurm1\n",
    "#JobAcctGatherFrequency=30\n",
    "JobAcctGatherType=jobacct_gather/none\n",
    "#SlurmctldDebug=3\n",
    "SlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log\n",
    "#SlurmdDebug=3\n",
    "SlurmdLogFile=/var/log/slurm-llnl/slurmd.log\n",
    "#\n",
    "#\n",
    "# COMPUTE NODES\n",
    "\"\"\".format(slurm_master=slurm_master)\n",
    "\n",
    "slurmconfstr += ' '.join((\"NodeName=DEFAULT\",\n",
    "                      \"Sockets=\"        + str(machine['sockets']),\n",
    "                      \"CoresPerSocket=\" + str(machine['cores']),\n",
    "                      \"ThreadsPerCore=\" + str(machine['threads']),\n",
    "                      \"State=UNKNOWN\"))\n",
    "\n",
    "slurmconfstr += '\\n'\n",
    "slurmconfstr += 'NodeName={slurm_master} \\n'.format(slurm_master=slurm_master)\n",
    "slurmconfstr += 'NodeName={compute_nodes} \\n'.format(compute_nodes=compute_nodes)\n",
    "\n",
    "\n",
    "slurmconfstr += \"\\n PartitionName=long Nodes={compute_nodes} Default=YES MaxTime=INFINITE State=UP \\n\".format(compute_nodes=compute_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# slurm.conf file generated by configurator easy.html.\n",
      "# Put this file on all nodes of your cluster.\n",
      "# See the slurm.conf man page for more information.\n",
      "#\n",
      "ControlMachine=ubslurm1\n",
      "AuthType=auth/none\n",
      "#CheckpointType=checkpoint/none\n",
      "#CryptoType=crypto/munge\n",
      "#ControlAddr=\n",
      "#\n",
      "#MailProg=/bin/mail\n",
      "MpiDefault=none\n",
      "#MpiParams=ports=#-#\n",
      "ProctrackType=proctrack/pgid\n",
      "ReturnToService=1\n",
      "SlurmctldPidFile=/var/run/slurm-llnl/slurmctld.pid\n",
      "#SlurmctldPort=6817\n",
      "SlurmdPidFile=/var/run/slurm-llnl/slurmd.pid\n",
      "#SlurmdPort=6818\n",
      "SlurmdSpoolDir=/var/lib/slurm-llnl/slurmd\n",
      "SlurmUser=slurm\n",
      "#SlurmdUser=root\n",
      "StateSaveLocation=/var/lib/slurm-llnl/slurmctld\n",
      "SwitchType=switch/none\n",
      "TaskPlugin=task/none\n",
      "JobCredentialPrivateKey=/home/prateek3_14/slurmkey\n",
      "#\n",
      "#\n",
      "# TIMERS\n",
      "#KillWait=30\n",
      "#MinJobAge=300\n",
      "#SlurmctldTimeout=120\n",
      "#SlurmdTimeout=300\n",
      "#\n",
      "#\n",
      "# SCHEDULING\n",
      "FastSchedule=1\n",
      "SchedulerType=sched/builtin\n",
      "#SchedulerPort=7321\n",
      "SelectType=select/linear\n",
      "#\n",
      "#\n",
      "# LOGGING AND ACCOUNTING\n",
      "AccountingStorageType=accounting_storage/none\n",
      "ClusterName=ubslurm1\n",
      "#JobAcctGatherFrequency=30\n",
      "JobAcctGatherType=jobacct_gather/none\n",
      "#SlurmctldDebug=3\n",
      "SlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log\n",
      "#SlurmdDebug=3\n",
      "SlurmdLogFile=/var/log/slurm-llnl/slurmd.log\n",
      "#\n",
      "#\n",
      "# COMPUTE NODES\n",
      "NodeName=DEFAULT Sockets=1 CoresPerSocket=1 ThreadsPerCore=2 State=UNKNOWN\n",
      "NodeName=ubslurm1 \n",
      "NodeName=bravo[1-10] \n",
      "\n",
      " PartitionName=long Nodes=bravo[1-10] Default=YES MaxTime=INFINITE State=UP \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print slurmconfstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "startupscriptstr = \"\"\"#!/bin/bash\n",
    "\n",
    "logger \"Startup Script Begins.... \"\n",
    "logger \"Running as `whoami`\" \n",
    "\n",
    "#systemctl stop slurmd \n",
    "\n",
    "cat <<\\EOF >> /etc/slurm-llnl/slurm.conf\n",
    "{slurmconfstr}\n",
    "EOF\n",
    "\n",
    "systemctl start slurmd \n",
    "\n",
    "logger \"Slurm conf applied, startup script ending\" \n",
    "\n",
    "exit 0\n",
    "\n",
    "\"\"\".format(slurmconfstr=slurmconfstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "logger \"Startup Script Begins.... \"\n",
      "logger \"Running as `whoami`\" \n",
      "\n",
      "#systemctl stop slurmd \n",
      "\n",
      "cat <<\\EOF >> /etc/slurm-llnl/slurm.conf\n",
      "\n",
      "# slurm.conf file generated by configurator easy.html.\n",
      "# Put this file on all nodes of your cluster.\n",
      "# See the slurm.conf man page for more information.\n",
      "#\n",
      "ControlMachine=ubslurm1\n",
      "AuthType=auth/none\n",
      "#CheckpointType=checkpoint/none\n",
      "#CryptoType=crypto/munge\n",
      "#ControlAddr=\n",
      "#\n",
      "#MailProg=/bin/mail\n",
      "MpiDefault=none\n",
      "#MpiParams=ports=#-#\n",
      "ProctrackType=proctrack/pgid\n",
      "ReturnToService=1\n",
      "SlurmctldPidFile=/var/run/slurm-llnl/slurmctld.pid\n",
      "#SlurmctldPort=6817\n",
      "SlurmdPidFile=/var/run/slurm-llnl/slurmd.pid\n",
      "#SlurmdPort=6818\n",
      "SlurmdSpoolDir=/var/lib/slurm-llnl/slurmd\n",
      "SlurmUser=slurm\n",
      "#SlurmdUser=root\n",
      "StateSaveLocation=/var/lib/slurm-llnl/slurmctld\n",
      "SwitchType=switch/none\n",
      "TaskPlugin=task/none\n",
      "JobCredentialPrivateKey=/home/prateek3_14/slurmkey\n",
      "#\n",
      "#\n",
      "# TIMERS\n",
      "#KillWait=30\n",
      "#MinJobAge=300\n",
      "#SlurmctldTimeout=120\n",
      "#SlurmdTimeout=300\n",
      "#\n",
      "#\n",
      "# SCHEDULING\n",
      "FastSchedule=1\n",
      "SchedulerType=sched/builtin\n",
      "#SchedulerPort=7321\n",
      "SelectType=select/linear\n",
      "#\n",
      "#\n",
      "# LOGGING AND ACCOUNTING\n",
      "AccountingStorageType=accounting_storage/none\n",
      "ClusterName=ubslurm1\n",
      "#JobAcctGatherFrequency=30\n",
      "JobAcctGatherType=jobacct_gather/none\n",
      "#SlurmctldDebug=3\n",
      "SlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log\n",
      "#SlurmdDebug=3\n",
      "SlurmdLogFile=/var/log/slurm-llnl/slurmd.log\n",
      "#\n",
      "#\n",
      "# COMPUTE NODES\n",
      "NodeName=DEFAULT Sockets=1 CoresPerSocket=1 ThreadsPerCore=2 State=UNKNOWN\n",
      "NodeName=ubslurm1 \n",
      "NodeName=bravo[1-10] \n",
      "\n",
      " PartitionName=long Nodes=bravo[1-10] Default=YES MaxTime=INFINITE State=UP \n",
      "\n",
      "EOF\n",
      "\n",
      "systemctl start slurmd \n",
      "\n",
      "logger \"Slurm conf applied, startup script ending\" \n",
      "\n",
      "exit 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(startupscriptstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "project='first-220321'\n",
    "ze='us-east1-b'\n",
    "vmnum=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute = googleapiclient.discovery.build('compute', 'v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'id': u'projects/first-220321/zones/us-central1-f/instances',\n",
       " u'items': [{u'cpuPlatform': u'Unknown CPU Platform',\n",
       "   u'creationTimestamp': u'2019-02-18T11:23:26.689-08:00',\n",
       "   u'deletionProtection': False,\n",
       "   u'disks': [{u'autoDelete': True,\n",
       "     u'boot': True,\n",
       "     u'deviceName': u'persistent-disk-0',\n",
       "     u'guestOsFeatures': [{u'type': u'VIRTIO_SCSI_MULTIQUEUE'}],\n",
       "     u'index': 0,\n",
       "     u'interface': u'SCSI',\n",
       "     u'kind': u'compute#attachedDisk',\n",
       "     u'licenses': [u'https://www.googleapis.com/compute/v1/projects/ubuntu-os-cloud/global/licenses/ubuntu-1804-lts'],\n",
       "     u'mode': u'READ_WRITE',\n",
       "     u'source': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/disks/aardvark10',\n",
       "     u'type': u'PERSISTENT'}],\n",
       "   u'id': u'8839863289518667970',\n",
       "   u'kind': u'compute#instance',\n",
       "   u'labelFingerprint': u'42WmSpB8rSM=',\n",
       "   u'machineType': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/machineTypes/n1-standard-16',\n",
       "   u'metadata': {u'fingerprint': u'fsDgias-br0=',\n",
       "    u'kind': u'compute#metadata'},\n",
       "   u'name': u'aardvark10',\n",
       "   u'networkInterfaces': [{u'accessConfigs': [{u'kind': u'compute#accessConfig',\n",
       "       u'name': u'External NAT',\n",
       "       u'networkTier': u'PREMIUM',\n",
       "       u'type': u'ONE_TO_ONE_NAT'}],\n",
       "     u'fingerprint': u'4KLtT71yTFo=',\n",
       "     u'kind': u'compute#networkInterface',\n",
       "     u'name': u'nic0',\n",
       "     u'network': u'https://www.googleapis.com/compute/v1/projects/first-220321/global/networks/default',\n",
       "     u'networkIP': u'10.128.0.5',\n",
       "     u'subnetwork': u'https://www.googleapis.com/compute/v1/projects/first-220321/regions/us-central1/subnetworks/default'}],\n",
       "   u'scheduling': {u'automaticRestart': False,\n",
       "    u'onHostMaintenance': u'TERMINATE',\n",
       "    u'preemptible': True},\n",
       "   u'selfLink': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/instances/aardvark10',\n",
       "   u'startRestricted': False,\n",
       "   u'status': u'TERMINATED',\n",
       "   u'tags': {u'fingerprint': u'42WmSpB8rSM='},\n",
       "   u'zone': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f'},\n",
       "  {u'cpuPlatform': u'Unknown CPU Platform',\n",
       "   u'creationTimestamp': u'2019-02-18T11:23:36.431-08:00',\n",
       "   u'deletionProtection': False,\n",
       "   u'disks': [{u'autoDelete': True,\n",
       "     u'boot': True,\n",
       "     u'deviceName': u'persistent-disk-0',\n",
       "     u'guestOsFeatures': [{u'type': u'VIRTIO_SCSI_MULTIQUEUE'}],\n",
       "     u'index': 0,\n",
       "     u'interface': u'SCSI',\n",
       "     u'kind': u'compute#attachedDisk',\n",
       "     u'licenses': [u'https://www.googleapis.com/compute/v1/projects/ubuntu-os-cloud/global/licenses/ubuntu-1804-lts'],\n",
       "     u'mode': u'READ_WRITE',\n",
       "     u'source': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/disks/aardvark11',\n",
       "     u'type': u'PERSISTENT'}],\n",
       "   u'id': u'92714280096807131',\n",
       "   u'kind': u'compute#instance',\n",
       "   u'labelFingerprint': u'42WmSpB8rSM=',\n",
       "   u'machineType': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/machineTypes/n1-standard-2',\n",
       "   u'metadata': {u'fingerprint': u'fsDgias-br0=',\n",
       "    u'kind': u'compute#metadata'},\n",
       "   u'name': u'aardvark11',\n",
       "   u'networkInterfaces': [{u'accessConfigs': [{u'kind': u'compute#accessConfig',\n",
       "       u'name': u'External NAT',\n",
       "       u'networkTier': u'PREMIUM',\n",
       "       u'type': u'ONE_TO_ONE_NAT'}],\n",
       "     u'fingerprint': u'8JaQrkbPA0Y=',\n",
       "     u'kind': u'compute#networkInterface',\n",
       "     u'name': u'nic0',\n",
       "     u'network': u'https://www.googleapis.com/compute/v1/projects/first-220321/global/networks/default',\n",
       "     u'networkIP': u'10.128.0.6',\n",
       "     u'subnetwork': u'https://www.googleapis.com/compute/v1/projects/first-220321/regions/us-central1/subnetworks/default'}],\n",
       "   u'scheduling': {u'automaticRestart': False,\n",
       "    u'onHostMaintenance': u'TERMINATE',\n",
       "    u'preemptible': True},\n",
       "   u'selfLink': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/instances/aardvark11',\n",
       "   u'startRestricted': False,\n",
       "   u'status': u'TERMINATED',\n",
       "   u'tags': {u'fingerprint': u'42WmSpB8rSM='},\n",
       "   u'zone': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f'},\n",
       "  {u'cpuPlatform': u'Unknown CPU Platform',\n",
       "   u'creationTimestamp': u'2019-02-18T11:23:43.468-08:00',\n",
       "   u'deletionProtection': False,\n",
       "   u'disks': [{u'autoDelete': True,\n",
       "     u'boot': True,\n",
       "     u'deviceName': u'persistent-disk-0',\n",
       "     u'guestOsFeatures': [{u'type': u'VIRTIO_SCSI_MULTIQUEUE'}],\n",
       "     u'index': 0,\n",
       "     u'interface': u'SCSI',\n",
       "     u'kind': u'compute#attachedDisk',\n",
       "     u'licenses': [u'https://www.googleapis.com/compute/v1/projects/ubuntu-os-cloud/global/licenses/ubuntu-1804-lts'],\n",
       "     u'mode': u'READ_WRITE',\n",
       "     u'source': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/disks/aardvark12',\n",
       "     u'type': u'PERSISTENT'}],\n",
       "   u'id': u'4343294042575386833',\n",
       "   u'kind': u'compute#instance',\n",
       "   u'labelFingerprint': u'42WmSpB8rSM=',\n",
       "   u'machineType': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/machineTypes/n1-standard-32',\n",
       "   u'metadata': {u'fingerprint': u'fsDgias-br0=',\n",
       "    u'kind': u'compute#metadata'},\n",
       "   u'name': u'aardvark12',\n",
       "   u'networkInterfaces': [{u'accessConfigs': [{u'kind': u'compute#accessConfig',\n",
       "       u'name': u'External NAT',\n",
       "       u'networkTier': u'PREMIUM',\n",
       "       u'type': u'ONE_TO_ONE_NAT'}],\n",
       "     u'fingerprint': u'AyX57gnkqYQ=',\n",
       "     u'kind': u'compute#networkInterface',\n",
       "     u'name': u'nic0',\n",
       "     u'network': u'https://www.googleapis.com/compute/v1/projects/first-220321/global/networks/default',\n",
       "     u'networkIP': u'10.128.0.7',\n",
       "     u'subnetwork': u'https://www.googleapis.com/compute/v1/projects/first-220321/regions/us-central1/subnetworks/default'}],\n",
       "   u'scheduling': {u'automaticRestart': False,\n",
       "    u'onHostMaintenance': u'TERMINATE',\n",
       "    u'preemptible': True},\n",
       "   u'selfLink': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/instances/aardvark12',\n",
       "   u'startRestricted': False,\n",
       "   u'status': u'TERMINATED',\n",
       "   u'tags': {u'fingerprint': u'42WmSpB8rSM='},\n",
       "   u'zone': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f'},\n",
       "  {u'cpuPlatform': u'Unknown CPU Platform',\n",
       "   u'creationTimestamp': u'2019-02-18T11:23:49.661-08:00',\n",
       "   u'deletionProtection': False,\n",
       "   u'disks': [{u'autoDelete': True,\n",
       "     u'boot': True,\n",
       "     u'deviceName': u'persistent-disk-0',\n",
       "     u'guestOsFeatures': [{u'type': u'VIRTIO_SCSI_MULTIQUEUE'}],\n",
       "     u'index': 0,\n",
       "     u'interface': u'SCSI',\n",
       "     u'kind': u'compute#attachedDisk',\n",
       "     u'licenses': [u'https://www.googleapis.com/compute/v1/projects/ubuntu-os-cloud/global/licenses/ubuntu-1804-lts'],\n",
       "     u'mode': u'READ_WRITE',\n",
       "     u'source': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/disks/aardvark13',\n",
       "     u'type': u'PERSISTENT'}],\n",
       "   u'id': u'1464418137406879914',\n",
       "   u'kind': u'compute#instance',\n",
       "   u'labelFingerprint': u'42WmSpB8rSM=',\n",
       "   u'machineType': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/machineTypes/n1-standard-4',\n",
       "   u'metadata': {u'fingerprint': u'fsDgias-br0=',\n",
       "    u'kind': u'compute#metadata'},\n",
       "   u'name': u'aardvark13',\n",
       "   u'networkInterfaces': [{u'accessConfigs': [{u'kind': u'compute#accessConfig',\n",
       "       u'name': u'External NAT',\n",
       "       u'networkTier': u'PREMIUM',\n",
       "       u'type': u'ONE_TO_ONE_NAT'}],\n",
       "     u'fingerprint': u's0NYYRTdgwM=',\n",
       "     u'kind': u'compute#networkInterface',\n",
       "     u'name': u'nic0',\n",
       "     u'network': u'https://www.googleapis.com/compute/v1/projects/first-220321/global/networks/default',\n",
       "     u'networkIP': u'10.128.0.8',\n",
       "     u'subnetwork': u'https://www.googleapis.com/compute/v1/projects/first-220321/regions/us-central1/subnetworks/default'}],\n",
       "   u'scheduling': {u'automaticRestart': False,\n",
       "    u'onHostMaintenance': u'TERMINATE',\n",
       "    u'preemptible': True},\n",
       "   u'selfLink': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/instances/aardvark13',\n",
       "   u'startRestricted': False,\n",
       "   u'status': u'TERMINATED',\n",
       "   u'tags': {u'fingerprint': u'42WmSpB8rSM='},\n",
       "   u'zone': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f'},\n",
       "  {u'cpuPlatform': u'Unknown CPU Platform',\n",
       "   u'creationTimestamp': u'2019-02-18T11:21:46.341-08:00',\n",
       "   u'deletionProtection': False,\n",
       "   u'disks': [{u'autoDelete': True,\n",
       "     u'boot': True,\n",
       "     u'deviceName': u'persistent-disk-0',\n",
       "     u'guestOsFeatures': [{u'type': u'VIRTIO_SCSI_MULTIQUEUE'}],\n",
       "     u'index': 0,\n",
       "     u'interface': u'SCSI',\n",
       "     u'kind': u'compute#attachedDisk',\n",
       "     u'licenses': [u'https://www.googleapis.com/compute/v1/projects/ubuntu-os-cloud/global/licenses/ubuntu-1804-lts'],\n",
       "     u'mode': u'READ_WRITE',\n",
       "     u'source': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/disks/aardvark8',\n",
       "     u'type': u'PERSISTENT'}],\n",
       "   u'id': u'7173384088538495782',\n",
       "   u'kind': u'compute#instance',\n",
       "   u'labelFingerprint': u'42WmSpB8rSM=',\n",
       "   u'machineType': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/machineTypes/n1-standard-1',\n",
       "   u'metadata': {u'fingerprint': u'fsDgias-br0=',\n",
       "    u'kind': u'compute#metadata'},\n",
       "   u'name': u'aardvark8',\n",
       "   u'networkInterfaces': [{u'accessConfigs': [{u'kind': u'compute#accessConfig',\n",
       "       u'name': u'External NAT',\n",
       "       u'networkTier': u'PREMIUM',\n",
       "       u'type': u'ONE_TO_ONE_NAT'}],\n",
       "     u'fingerprint': u'bPjhS2CB-Hw=',\n",
       "     u'kind': u'compute#networkInterface',\n",
       "     u'name': u'nic0',\n",
       "     u'network': u'https://www.googleapis.com/compute/v1/projects/first-220321/global/networks/default',\n",
       "     u'networkIP': u'10.128.0.3',\n",
       "     u'subnetwork': u'https://www.googleapis.com/compute/v1/projects/first-220321/regions/us-central1/subnetworks/default'}],\n",
       "   u'scheduling': {u'automaticRestart': False,\n",
       "    u'onHostMaintenance': u'TERMINATE',\n",
       "    u'preemptible': True},\n",
       "   u'selfLink': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/instances/aardvark8',\n",
       "   u'startRestricted': False,\n",
       "   u'status': u'TERMINATED',\n",
       "   u'tags': {u'fingerprint': u'42WmSpB8rSM='},\n",
       "   u'zone': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f'},\n",
       "  {u'cpuPlatform': u'Unknown CPU Platform',\n",
       "   u'creationTimestamp': u'2019-02-18T11:23:20.163-08:00',\n",
       "   u'deletionProtection': False,\n",
       "   u'disks': [{u'autoDelete': True,\n",
       "     u'boot': True,\n",
       "     u'deviceName': u'persistent-disk-0',\n",
       "     u'guestOsFeatures': [{u'type': u'VIRTIO_SCSI_MULTIQUEUE'}],\n",
       "     u'index': 0,\n",
       "     u'interface': u'SCSI',\n",
       "     u'kind': u'compute#attachedDisk',\n",
       "     u'licenses': [u'https://www.googleapis.com/compute/v1/projects/ubuntu-os-cloud/global/licenses/ubuntu-1804-lts'],\n",
       "     u'mode': u'READ_WRITE',\n",
       "     u'source': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/disks/aardvark9',\n",
       "     u'type': u'PERSISTENT'}],\n",
       "   u'id': u'3276993043257276616',\n",
       "   u'kind': u'compute#instance',\n",
       "   u'labelFingerprint': u'42WmSpB8rSM=',\n",
       "   u'machineType': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/machineTypes/n1-standard-1',\n",
       "   u'metadata': {u'fingerprint': u'fsDgias-br0=',\n",
       "    u'kind': u'compute#metadata'},\n",
       "   u'name': u'aardvark9',\n",
       "   u'networkInterfaces': [{u'accessConfigs': [{u'kind': u'compute#accessConfig',\n",
       "       u'name': u'External NAT',\n",
       "       u'networkTier': u'PREMIUM',\n",
       "       u'type': u'ONE_TO_ONE_NAT'}],\n",
       "     u'fingerprint': u'7sXzIn5WELM=',\n",
       "     u'kind': u'compute#networkInterface',\n",
       "     u'name': u'nic0',\n",
       "     u'network': u'https://www.googleapis.com/compute/v1/projects/first-220321/global/networks/default',\n",
       "     u'networkIP': u'10.128.0.4',\n",
       "     u'subnetwork': u'https://www.googleapis.com/compute/v1/projects/first-220321/regions/us-central1/subnetworks/default'}],\n",
       "   u'scheduling': {u'automaticRestart': False,\n",
       "    u'onHostMaintenance': u'TERMINATE',\n",
       "    u'preemptible': True},\n",
       "   u'selfLink': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/instances/aardvark9',\n",
       "   u'startRestricted': False,\n",
       "   u'status': u'TERMINATED',\n",
       "   u'tags': {u'fingerprint': u'42WmSpB8rSM='},\n",
       "   u'zone': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f'},\n",
       "  {u'cpuPlatform': u'Unknown CPU Platform',\n",
       "   u'creationTimestamp': u'2019-02-18T08:57:37.119-08:00',\n",
       "   u'deletionProtection': False,\n",
       "   u'disks': [{u'autoDelete': False,\n",
       "     u'boot': True,\n",
       "     u'deviceName': u'persistent-disk-0',\n",
       "     u'guestOsFeatures': [{u'type': u'VIRTIO_SCSI_MULTIQUEUE'}],\n",
       "     u'index': 0,\n",
       "     u'interface': u'SCSI',\n",
       "     u'kind': u'compute#attachedDisk',\n",
       "     u'licenses': [u'https://www.googleapis.com/compute/v1/projects/ubuntu-os-cloud/global/licenses/ubuntu-1804-lts'],\n",
       "     u'mode': u'READ_WRITE',\n",
       "     u'source': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/disks/disk-2',\n",
       "     u'type': u'PERSISTENT'}],\n",
       "   u'id': u'6560694810351983343',\n",
       "   u'kind': u'compute#instance',\n",
       "   u'labelFingerprint': u'42WmSpB8rSM=',\n",
       "   u'machineType': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/machineTypes/n1-standard-2',\n",
       "   u'metadata': {u'fingerprint': u'fsDgias-br0=',\n",
       "    u'kind': u'compute#metadata'},\n",
       "   u'name': u'api-i1',\n",
       "   u'networkInterfaces': [{u'accessConfigs': [{u'kind': u'compute#accessConfig',\n",
       "       u'name': u'External NAT',\n",
       "       u'networkTier': u'PREMIUM',\n",
       "       u'type': u'ONE_TO_ONE_NAT'}],\n",
       "     u'fingerprint': u'MWviZCTw-Gc=',\n",
       "     u'kind': u'compute#networkInterface',\n",
       "     u'name': u'nic0',\n",
       "     u'network': u'https://www.googleapis.com/compute/v1/projects/first-220321/global/networks/default',\n",
       "     u'networkIP': u'10.128.0.2',\n",
       "     u'subnetwork': u'https://www.googleapis.com/compute/v1/projects/first-220321/regions/us-central1/subnetworks/default'}],\n",
       "   u'scheduling': {u'automaticRestart': False,\n",
       "    u'onHostMaintenance': u'TERMINATE',\n",
       "    u'preemptible': True},\n",
       "   u'selfLink': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/instances/api-i1',\n",
       "   u'startRestricted': False,\n",
       "   u'status': u'TERMINATED',\n",
       "   u'tags': {u'fingerprint': u'42WmSpB8rSM='},\n",
       "   u'zone': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f'},\n",
       "  {u'cpuPlatform': u'Intel Ivy Bridge',\n",
       "   u'creationTimestamp': u'2019-02-19T14:02:58.378-08:00',\n",
       "   u'deletionProtection': False,\n",
       "   u'disks': [{u'autoDelete': True,\n",
       "     u'boot': True,\n",
       "     u'deviceName': u'persistent-disk-0',\n",
       "     u'guestOsFeatures': [{u'type': u'VIRTIO_SCSI_MULTIQUEUE'}],\n",
       "     u'index': 0,\n",
       "     u'interface': u'SCSI',\n",
       "     u'kind': u'compute#attachedDisk',\n",
       "     u'licenses': [u'https://www.googleapis.com/compute/v1/projects/ubuntu-os-cloud/global/licenses/ubuntu-1804-lts'],\n",
       "     u'mode': u'READ_WRITE',\n",
       "     u'source': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/disks/bravo1',\n",
       "     u'type': u'PERSISTENT'}],\n",
       "   u'id': u'4919679933598516222',\n",
       "   u'kind': u'compute#instance',\n",
       "   u'labelFingerprint': u'42WmSpB8rSM=',\n",
       "   u'machineType': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/machineTypes/n1-standard-2',\n",
       "   u'metadata': {u'fingerprint': u'HQi3ZxGlLy0=',\n",
       "    u'items': [{u'key': u'startup-script',\n",
       "      u'value': u'#!/bin/bash\\n\\nlogger \"Startup Script Begins.... \"\\nlogger \"Running as `whoami`\" \\n\\n#systemctl stop slurmd \\n\\ncat <<\\\\EOF >> /etc/slurm-llnl/slurm.conf\\n\\n# slurm.conf file generated by configurator easy.html.\\n# Put this file on all nodes of your cluster.\\n# See the slurm.conf man page for more information.\\n#\\nControlMachine=ubslurm1\\nAuthType=auth/none\\n#CheckpointType=checkpoint/none\\n#CryptoType=crypto/munge\\n#ControlAddr=\\n#\\n#MailProg=/bin/mail\\nMpiDefault=none\\n#MpiParams=ports=#-#\\nProctrackType=proctrack/pgid\\nReturnToService=1\\nSlurmctldPidFile=/var/run/slurm-llnl/slurmctld.pid\\n#SlurmctldPort=6817\\nSlurmdPidFile=/var/run/slurm-llnl/slurmd.pid\\n#SlurmdPort=6818\\nSlurmdSpoolDir=/var/lib/slurm-llnl/slurmd\\nSlurmUser=slurm\\n#SlurmdUser=root\\nStateSaveLocation=/var/lib/slurm-llnl/slurmctld\\nSwitchType=switch/none\\nTaskPlugin=task/none\\nJobCredentialPrivateKey=/home/prateek3_14/slurmkey\\n#\\n#\\n# TIMERS\\n#KillWait=30\\n#MinJobAge=300\\n#SlurmctldTimeout=120\\n#SlurmdTimeout=300\\n#\\n#\\n# SCHEDULING\\nFastSchedule=1\\nSchedulerType=sched/builtin\\n#SchedulerPort=7321\\nSelectType=select/linear\\n#\\n#\\n# LOGGING AND ACCOUNTING\\nAccountingStorageType=accounting_storage/none\\nClusterName=ubslurm1\\n#JobAcctGatherFrequency=30\\nJobAcctGatherType=jobacct_gather/none\\n#SlurmctldDebug=3\\nSlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log\\n#SlurmdDebug=3\\nSlurmdLogFile=/var/log/slurm-llnl/slurmd.log\\n#\\n#\\n# COMPUTE NODES\\nNodeName=DEFAULT Sockets=1 CoresPerSocket=1 ThreadsPerCore=2 State=UNKNOWN\\nNodeName=ubslurm1 \\nNodeName=bravo[1-10] \\n\\n PartitionName=long Nodes=bravo[1-10] Default=YES MaxTime=INFINITE State=UP \\n\\nEOF\\n\\nsystemctl start slurmd \\n\\nlogger \"Slurm conf applied, startup script ending\" \\n\\nexit 0\\n\\n'}],\n",
       "    u'kind': u'compute#metadata'},\n",
       "   u'name': u'bravo1',\n",
       "   u'networkInterfaces': [{u'accessConfigs': [{u'kind': u'compute#accessConfig',\n",
       "       u'name': u'External NAT',\n",
       "       u'natIP': u'35.193.54.140',\n",
       "       u'networkTier': u'PREMIUM',\n",
       "       u'type': u'ONE_TO_ONE_NAT'}],\n",
       "     u'fingerprint': u'NtsplXmvRQQ=',\n",
       "     u'kind': u'compute#networkInterface',\n",
       "     u'name': u'nic0',\n",
       "     u'network': u'https://www.googleapis.com/compute/v1/projects/first-220321/global/networks/default',\n",
       "     u'networkIP': u'10.128.0.9',\n",
       "     u'subnetwork': u'https://www.googleapis.com/compute/v1/projects/first-220321/regions/us-central1/subnetworks/default'}],\n",
       "   u'scheduling': {u'automaticRestart': False,\n",
       "    u'onHostMaintenance': u'TERMINATE',\n",
       "    u'preemptible': True},\n",
       "   u'selfLink': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/instances/bravo1',\n",
       "   u'startRestricted': False,\n",
       "   u'status': u'RUNNING',\n",
       "   u'tags': {u'fingerprint': u'42WmSpB8rSM='},\n",
       "   u'zone': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f'},\n",
       "  {u'cpuPlatform': u'Intel Ivy Bridge',\n",
       "   u'creationTimestamp': u'2019-02-19T05:52:15.611-08:00',\n",
       "   u'deletionProtection': False,\n",
       "   u'disks': [{u'autoDelete': True,\n",
       "     u'boot': True,\n",
       "     u'deviceName': u'boot',\n",
       "     u'index': 0,\n",
       "     u'interface': u'SCSI',\n",
       "     u'kind': u'compute#attachedDisk',\n",
       "     u'licenses': [u'https://www.googleapis.com/compute/v1/projects/centos-cloud/global/licenses/centos-7'],\n",
       "     u'mode': u'READ_WRITE',\n",
       "     u'source': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/disks/sl1-compute1',\n",
       "     u'type': u'PERSISTENT'}],\n",
       "   u'id': u'2333820733316248801',\n",
       "   u'kind': u'compute#instance',\n",
       "   u'labelFingerprint': u'sfYKEHt6udQ=',\n",
       "   u'labels': {u'goog-dm': u'slurm', u'sl1-compute1': u'slurm-on-gcp'},\n",
       "   u'machineType': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/machineTypes/n1-standard-2',\n",
       "   u'metadata': {u'fingerprint': u'vnAAFuBF0Q0=',\n",
       "    u'items': [{u'key': u'enable-oslogin', u'value': u'TRUE'}],\n",
       "    u'kind': u'compute#metadata'},\n",
       "   u'name': u'sl1-compute1',\n",
       "   u'networkInterfaces': [{u'accessConfigs': [{u'kind': u'compute#accessConfig',\n",
       "       u'name': u'External NAT',\n",
       "       u'natIP': u'35.184.158.52',\n",
       "       u'networkTier': u'PREMIUM',\n",
       "       u'type': u'ONE_TO_ONE_NAT'}],\n",
       "     u'fingerprint': u'CPzndINRI0g=',\n",
       "     u'kind': u'compute#networkInterface',\n",
       "     u'name': u'nic0',\n",
       "     u'network': u'https://www.googleapis.com/compute/v1/projects/first-220321/global/networks/sl1-slurm-network',\n",
       "     u'networkIP': u'10.10.0.2',\n",
       "     u'subnetwork': u'https://www.googleapis.com/compute/v1/projects/first-220321/regions/us-central1/subnetworks/sl1-slurm-subnet'}],\n",
       "   u'scheduling': {u'automaticRestart': True,\n",
       "    u'onHostMaintenance': u'MIGRATE',\n",
       "    u'preemptible': False},\n",
       "   u'selfLink': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/instances/sl1-compute1',\n",
       "   u'serviceAccounts': [{u'email': u'316905473554-compute@developer.gserviceaccount.com',\n",
       "     u'scopes': [u'https://www.googleapis.com/auth/cloud-platform']}],\n",
       "   u'startRestricted': False,\n",
       "   u'status': u'RUNNING',\n",
       "   u'tags': {u'fingerprint': u'micHwuEN8fk=', u'items': [u'compute']},\n",
       "   u'zone': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f'},\n",
       "  {u'cpuPlatform': u'Unknown CPU Platform',\n",
       "   u'creationTimestamp': u'2019-02-19T05:52:16.102-08:00',\n",
       "   u'deletionProtection': False,\n",
       "   u'disks': [{u'autoDelete': True,\n",
       "     u'boot': True,\n",
       "     u'deviceName': u'boot',\n",
       "     u'index': 0,\n",
       "     u'interface': u'SCSI',\n",
       "     u'kind': u'compute#attachedDisk',\n",
       "     u'licenses': [u'https://www.googleapis.com/compute/v1/projects/centos-cloud/global/licenses/centos-7'],\n",
       "     u'mode': u'READ_WRITE',\n",
       "     u'source': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/disks/sl1-compute2',\n",
       "     u'type': u'PERSISTENT'}],\n",
       "   u'id': u'5314678884581618913',\n",
       "   u'kind': u'compute#instance',\n",
       "   u'labelFingerprint': u'G6AkVxTIaWE=',\n",
       "   u'labels': {u'goog-dm': u'slurm', u'sl1-compute2': u'slurm-on-gcp'},\n",
       "   u'machineType': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/machineTypes/n1-standard-2',\n",
       "   u'metadata': {u'fingerprint': u'vnAAFuBF0Q0=',\n",
       "    u'items': [{u'key': u'enable-oslogin', u'value': u'TRUE'}],\n",
       "    u'kind': u'compute#metadata'},\n",
       "   u'name': u'sl1-compute2',\n",
       "   u'networkInterfaces': [{u'accessConfigs': [{u'kind': u'compute#accessConfig',\n",
       "       u'name': u'External NAT',\n",
       "       u'networkTier': u'PREMIUM',\n",
       "       u'type': u'ONE_TO_ONE_NAT'}],\n",
       "     u'fingerprint': u'-FKiYXvAUXs=',\n",
       "     u'kind': u'compute#networkInterface',\n",
       "     u'name': u'nic0',\n",
       "     u'network': u'https://www.googleapis.com/compute/v1/projects/first-220321/global/networks/sl1-slurm-network',\n",
       "     u'networkIP': u'10.10.0.3',\n",
       "     u'subnetwork': u'https://www.googleapis.com/compute/v1/projects/first-220321/regions/us-central1/subnetworks/sl1-slurm-subnet'}],\n",
       "   u'scheduling': {u'automaticRestart': True,\n",
       "    u'onHostMaintenance': u'MIGRATE',\n",
       "    u'preemptible': False},\n",
       "   u'selfLink': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/instances/sl1-compute2',\n",
       "   u'serviceAccounts': [{u'email': u'316905473554-compute@developer.gserviceaccount.com',\n",
       "     u'scopes': [u'https://www.googleapis.com/auth/cloud-platform']}],\n",
       "   u'startRestricted': False,\n",
       "   u'status': u'TERMINATED',\n",
       "   u'tags': {u'fingerprint': u'micHwuEN8fk=', u'items': [u'compute']},\n",
       "   u'zone': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f'},\n",
       "  {u'cpuPlatform': u'Intel Ivy Bridge',\n",
       "   u'creationTimestamp': u'2019-02-19T05:52:16.485-08:00',\n",
       "   u'deletionProtection': False,\n",
       "   u'disks': [{u'autoDelete': True,\n",
       "     u'boot': True,\n",
       "     u'deviceName': u'boot',\n",
       "     u'index': 0,\n",
       "     u'interface': u'SCSI',\n",
       "     u'kind': u'compute#attachedDisk',\n",
       "     u'licenses': [u'https://www.googleapis.com/compute/v1/projects/centos-cloud/global/licenses/centos-7'],\n",
       "     u'mode': u'READ_WRITE',\n",
       "     u'source': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/disks/sl1-controller',\n",
       "     u'type': u'PERSISTENT'}],\n",
       "   u'id': u'7793120484639487201',\n",
       "   u'kind': u'compute#instance',\n",
       "   u'labelFingerprint': u'm1apLtBTzfY=',\n",
       "   u'labels': {u'goog-dm': u'slurm', u'sl1-controller': u'slurm-on-gcp'},\n",
       "   u'machineType': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/machineTypes/n1-standard-2',\n",
       "   u'metadata': {u'fingerprint': u'Sl2F52iDWtA=',\n",
       "    u'items': [{u'key': u'enable-oslogin', u'value': u'TRUE'},\n",
       "     {u'key': u'slurm_resume',\n",
       "      u'value': u'#!/usr/bin/python\\n\\n# Copyright 2017 SchedMD LLC.\\n# Modified for use with the Slurm Resource Manager.\\n#\\n# Copyright 2015 Google Inc. All rights reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\nimport argparse\\nimport logging\\nimport shlex\\nimport subprocess\\nimport time\\n\\nimport googleapiclient.discovery\\n\\nCLUSTER_NAME = \\'sl1\\'\\n\\nPROJECT      = \\'first-220321\\'\\nZONE         = \\'us-central1-f\\'\\nREGION       = \\'us-central1\\'\\nMACHINE_TYPE = \\'n1-standard-2\\'\\nCPU_PLATFORM = \\'\\'\\nPREEMPTIBLE  = False\\nEXTERNAL_IP  = True\\nSHARED_VPC_HOST_PROJ = \\'\\'\\nVPC_SUBNET   = \\'\\'\\n\\nDISK_SIZE_GB = \\'100\\'\\nDISK_TYPE    = \\'pd-standard\\'\\n\\nLABELS       = \\'slurm-on-gcp\\'\\n\\nNETWORK_TYPE = \\'subnetwork\\'\\nNETWORK      = \"projects/%s/regions/%s/subnetworks/%s-slurm-subnet\" % (PROJECT, REGION, CLUSTER_NAME)\\n\\nGPU_TYPE     = \\'\\'\\nGPU_COUNT    = \\'0\\'\\n\\nSCONTROL     = \\'/apps/slurm/current/bin/scontrol\\'\\nLOGFILE      = \\'/apps/slurm/log/resume.log\\'\\n\\n# [START create_instance]\\ndef create_instance(compute, project, zone, instance_type, instance_name):\\n  # Get the latest CentOS 7image.\\n  image_response = compute.images().getFromFamily(\\n    project=\\'centos-cloud\\', family=\\'centos-7\\').execute()\\n  source_disk_image = image_response[\\'selfLink\\']\\n\\n  # Configure the machine\\n  machine_type = \"zones/%s/machineTypes/%s\" % (zone, instance_type)\\n  disk_type = \"projects/%s/zones/%s/diskTypes/%s\" % (PROJECT, ZONE, DISK_TYPE)\\n  startup_script = open(\\'/apps/slurm/scripts/startup-script.py\\', \\'r\\').read()\\n\\n  config = {\\n    \\'name\\': instance_name,\\n    \\'machineType\\': machine_type,\\n\\n    # Specify the boot disk and the image to use as a source.\\n    \\'disks\\': [\\n      {\\n        \\'boot\\': True,\\n        \\'autoDelete\\': True,\\n        \\'initializeParams\\': {\\n          \\'sourceImage\\': source_disk_image,\\n          \\'diskType\\': disk_type,\\n          \\'diskSizeGb\\': DISK_SIZE_GB\\n        }\\n      }\\n    ],\\n\\n    # Specify a network interface\\n    \\'networkInterfaces\\': [{\\n      NETWORK_TYPE : NETWORK,\\n    }],\\n\\n    # Allow the instance to access cloud storage and logging.\\n    \\'serviceAccounts\\': [{\\n      \\'email\\': \\'default\\',\\n      \\'scopes\\': [\\n        \\'https://www.googleapis.com/auth/cloud-platform\\'\\n      ]\\n    }],\\n\\n    \\'tags\\': {\\'items\\': [\\'compute\\'] },\\n\\n    # Metadata is readable from the instance and allows you to\\n    # pass configuration from deployment scripts to instances.\\n    \\'metadata\\': {\\n      \\'items\\': [{\\n        # Startup script is automatically executed by the\\n        # instance upon startup.\\n        \\'key\\': \\'startup-script\\',\\n        \\'value\\': startup_script\\n      }, {\\n        \\'key\\': \\'enable-oslogin\\',\\n        \\'value\\': \\'TRUE\\'\\n      }]\\n    }\\n  }\\n\\n  if GPU_TYPE:\\n    config[\\'guestAccelerators\\'] = [{\\n        \\'acceleratorCount\\': GPU_COUNT,\\n        \\'acceleratorType\\' : \\'https://www.googleapis.com/compute/v1/projects/\\' + PROJECT + \\'/zones/\\' + ZONE + \\'/acceleratorTypes/\\' + GPU_TYPE\\n        }]\\n\\n    config[\\'scheduling\\'] = {\\'onHostMaintenance\\': \\'TERMINATE\\'}\\n\\n  if PREEMPTIBLE:\\n      config[\\'scheduling\\'] = {\\n              \"preemptible\": True,\\n              \"onHostMaintenance\": \"TERMINATE\",\\n              \"automaticRestart\": False\\n              },\\n\\n  if LABELS:\\n      config[\\'labels\\'] = {instance_name: LABELS},\\n\\n  if CPU_PLATFORM:\\n      config[\\'minCpuPlatform\\'] = CPU_PLATFORM,\\n\\n  if VPC_SUBNET:\\n      config[\\'networkInterfaces\\'] = [{\\n      \\tNETWORK_TYPE : \"projects/%s/regions/%s/subnetworks/%s\" % (PROJECT, REGION, VPC_SUBNET)\\n      }]\\n\\n  if SHARED_VPC_HOST_PROJ:\\n      config[\\'networkInterfaces\\'] = [{\\n      \\tNETWORK_TYPE : \"projects/%s/regions/%s/subnetworks/%s\" % (SHARED_VPC_HOST_PROJ, REGION, VPC_SUBNET)\\n      }]\\n\\n  if EXTERNAL_IP or SHARED_VPC_HOST_PROJ:\\n      config[\\'networkInterfaces\\'][0][\\'accessConfigs\\'] = [\\n                {\\'type\\': \\'ONE_TO_ONE_NAT\\', \\'name\\': \\'External NAT\\'}\\n             ]\\n\\n\\n  return compute.instances().insert(\\n    project=project,\\n    zone=zone,\\n    body=config).execute()\\n# [END create_instance]\\n\\n# [START wait_for_operation]\\ndef wait_for_operation(compute, project, zone, operation):\\n    print(\\'Waiting for operation to finish...\\')\\n    while True:\\n        result = compute.zoneOperations().get(\\n            project=project,\\n            zone=zone,\\n            operation=operation).execute()\\n\\n        if result[\\'status\\'] == \\'DONE\\':\\n            print(\"done.\")\\n            if \\'error\\' in result:\\n                raise Exception(result[\\'error\\'])\\n            return result\\n\\n        time.sleep(1)\\n# [END wait_for_operation]\\n\\n# [START main]\\ndef main(short_node_list):\\n  logging.info(\"Bursting out:\" + short_node_list)\\n  compute = googleapiclient.discovery.build(\\'compute\\', \\'v1\\',\\n                                            cache_discovery=False)\\n\\n  # Get node list\\n  show_hostname_cmd = \"%s show hostname %s\" % (SCONTROL, short_node_list)\\n  node_list = subprocess.check_output(shlex.split(show_hostname_cmd))\\n\\n  operations = {}\\n  for node_name in node_list.splitlines():\\n    try:\\n      instance = compute.instances().get(\\n                project=PROJECT, zone=ZONE, instance=node_name,\\n                fields=\\'name,status\\').execute()\\n      logging.info(\"node %s already exists in state %s\" % (node_name,\\n          instance[\\'status\\']))\\n      operations[node_name] = compute.instances().start(project=PROJECT,\\n              zone=ZONE, instance=node_name).execute()\\n      logging.info(\"Sent start instance for \" + node_name)\\n    except:\\n      try:\\n        operations[node_name] = create_instance(compute, PROJECT, ZONE,\\n                                                MACHINE_TYPE, node_name)\\n        logging.info(\"Sent create instance for \" + node_name)\\n      except Exception, e:\\n        logging.exception(\"Error in creation of %s (%s)\" % (node_name, str(e)))\\n        cmd = \"%s update node=%s state=down reason=\\'%s\\'\" % \\\\\\n              (SCONTROL, node_name, str(e))\\n        subprocess.call(shlex.split(cmd))\\n\\n  for node_name in operations:\\n    try:\\n      operation = operations[node_name]\\n      # Do this after the instances have been initialized and then wait for all\\n      # operations to finish. Then updates their addrs.\\n      wait_for_operation(compute, PROJECT, ZONE, operation[\\'name\\'])\\n\\n      instance_networks = compute.instances().get(\\n                project=PROJECT, zone=ZONE, instance=node_name,\\n                fields=\\'networkInterfaces(name,network,networkIP,subnetwork)\\'\\n                ).execute()\\n      instance_ip = instance_networks[\\'networkInterfaces\\'][0][\\'networkIP\\']\\n\\n      node_update_cmd = \"%s update node=%s nodeaddr=%s\" % \\\\\\n              (SCONTROL, node_name, instance_ip)\\n      subprocess.call(shlex.split(node_update_cmd))\\n\\n      logging.info(\"Instance \" + node_name + \" is now up\")\\n    except Exception, e:\\n      logging.exception(\"Error in adding %s to slurm (%s)\" % (node_name, str(e)))\\n\\n# [END main]\\n\\n\\nif __name__ == \\'__main__\\':\\n  parser = argparse.ArgumentParser(\\n    description=__doc__,\\n    formatter_class=argparse.RawDescriptionHelpFormatter)\\n  parser.add_argument(\\'nodes\\', help=\\'Nodes to burst\\')\\n\\n  args = parser.parse_args()\\n  logging.basicConfig(filename=LOGFILE,\\n                      format=\\'%(asctime)s %(name)s %(levelname)s: %(message)s\\',\\n                      level=logging.DEBUG)\\n\\n  main(args.nodes)\\n'},\n",
       "     {u'key': u'slurm_suspend',\n",
       "      u'value': u'#!/usr/bin/env python\\n\\n# Copyright 2017 SchedMD LLC.\\n# Modified for use with the Slurm Resource Manager.\\n#\\n# Copyright 2015 Google Inc. All rights reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\nimport argparse\\nimport logging\\nimport shlex\\nimport subprocess\\nimport time\\n\\nimport googleapiclient.discovery\\n\\nPROJECT      = \\'first-220321\\'\\nZONE         = \\'us-central1-f\\'\\nSCONTROL     = \\'/apps/slurm/current/bin/scontrol\\'\\nLOGFILE      = \\'/apps/slurm/log/suspend.log\\'\\n\\n# [START delete_instance]\\ndef delete_instance(compute, project, zone, node_name):\\n  return compute.instances().delete(\\n    project=project,\\n    zone=zone,\\n    instance=node_name).execute()\\n# [END delete_instance]\\n\\n# [START wait_for_operation]\\ndef wait_for_operation(compute, project, zone, operation):\\n    print(\\'Waiting for operation to finish...\\')\\n    while True:\\n        result = compute.zoneOperations().get(\\n            project=project,\\n            zone=zone,\\n            operation=operation).execute()\\n\\n        if result[\\'status\\'] == \\'DONE\\':\\n            print(\"done.\")\\n            if \\'error\\' in result:\\n                raise Exception(result[\\'error\\'])\\n            return result\\n\\n        time.sleep(1)\\n# [END wait_for_operation]\\n\\n# [START main]\\ndef main(short_node_list):\\n  logging.info(\"Releasing nodes:\" + short_node_list)\\n  compute = googleapiclient.discovery.build(\\'compute\\', \\'v1\\',\\n                                            cache_discovery=False)\\n\\n  # Get node list\\n  show_hostname_cmd = \"%s show hostname %s\" % (SCONTROL, short_node_list)\\n  node_list = subprocess.check_output(shlex.split(show_hostname_cmd))\\n\\n  operations = {}\\n  for node_name in node_list.splitlines():\\n    try:\\n      operations[node_name] = delete_instance(compute, PROJECT, ZONE, node_name)\\n    except Exception, e:\\n      logging.exception(\"error during release of %s (%s)\" % (node_name, str(e)))\\n\\n  for node_name in operations:\\n    operation = operations[node_name]\\n    try:\\n      # Do we care if they have completely deleted? Waiting will cause it to\\n      # wait for each to be completely deleted befotre the next delete is made.\\n      # Could issue all deletes and then wait for the deletes to finish..\\n      wait_for_operation(compute, PROJECT, ZONE, operation[\\'name\\'])\\n      logging.info(\"deleted instance \" + node_name)\\n    except Exception, e:\\n      logging.exception(\"error deleting %s (%s)\" % (node_name, str(e)))\\n\\n  logging.info(\"done deleting instances\")\\n\\n# [END main]\\n\\n\\nif __name__ == \\'__main__\\':\\n  parser = argparse.ArgumentParser(\\n    description=__doc__,\\n    formatter_class=argparse.RawDescriptionHelpFormatter)\\n  parser.add_argument(\\'nodes\\', help=\\'Nodes to release\\')\\n\\n  args = parser.parse_args()\\n  logging.basicConfig(filename=LOGFILE,\\n                      format=\\'%(asctime)s %(name)s %(levelname)s: %(message)s\\',\\n                      level=logging.DEBUG)\\n\\n  main(args.nodes)\\n'},\n",
       "     {u'key': u'startup-script-compute',\n",
       "      u'value': u'#!/usr/bin/python\\n\\n# Copyright 2017 SchedMD LLC.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\nimport httplib\\nimport os\\nimport shlex\\nimport socket\\nimport subprocess\\nimport time\\nimport urllib\\nimport urllib2\\n\\nCLUSTER_NAME      = \\'sl1\\'\\nMACHINE_TYPE      = \\'n1-standard-2\\' # e.g. n1-standard-1, n1-starndard-2\\nINSTANCE_TYPE     = \\'compute\\' # e.g. controller, login, compute\\n\\nPROJECT           = \\'first-220321\\'\\nZONE              = \\'us-central1-f\\'\\n\\nAPPS_DIR          = \\'/apps\\'\\nMUNGE_DIR         = \"/etc/munge\"\\nMUNGE_KEY         = \\'\\'\\nSLURM_VERSION     = \\'18.08.5-2\\'\\nSTATIC_NODE_COUNT = 2\\nMAX_NODE_COUNT    = 10\\nDEF_SLURM_ACCT    = \\'default\\'\\nDEF_SLURM_USERS   = \\'prateeks\\'\\nEXTERNAL_COMPUTE_IPS = True\\nGPU_TYPE          = \\'\\'\\nGPU_COUNT         = 0\\nNFS_APPS_SERVER   = \\'\\'\\nNFS_HOME_SERVER   = \\'\\'\\nCONTROLLER_SECONDARY_DISK = False\\nSEC_DISK_DIR      = \\'/mnt/disks/sec\\'\\n\\nCONTROL_MACHINE = CLUSTER_NAME + \\'-controller\\'\\n\\nSLURM_PREFIX  = APPS_DIR + \\'/slurm/slurm-\\' + SLURM_VERSION\\n\\nMOTD_HEADER = \\'\\'\\'\\n\\n                                 SSSSSSS\\n                                SSSSSSSSS\\n                                SSSSSSSSS\\n                                SSSSSSSSS\\n                        SSSS     SSSSSSS     SSSS\\n                       SSSSSS               SSSSSS\\n                       SSSSSS    SSSSSSS    SSSSSS\\n                        SSSS    SSSSSSSSS    SSSS\\n                SSS             SSSSSSSSS             SSS\\n               SSSSS    SSSS    SSSSSSSSS    SSSS    SSSSS\\n                SSS    SSSSSS   SSSSSSSSS   SSSSSS    SSS\\n                       SSSSSS    SSSSSSS    SSSSSS\\n                SSS    SSSSSS               SSSSSS    SSS\\n               SSSSS    SSSS     SSSSSSS     SSSS    SSSSS\\n          S     SSS             SSSSSSSSS             SSS     S\\n         SSS            SSSS    SSSSSSSSS    SSSS            SSS\\n          S     SSS    SSSSSS   SSSSSSSSS   SSSSSS    SSS     S\\n               SSSSS   SSSSSS   SSSSSSSSS   SSSSSS   SSSSS\\n          S    SSSSS    SSSS     SSSSSSS     SSSS    SSSSS    S\\n    S    SSS    SSS                                   SSS    SSS    S\\n    S     S                                                   S     S\\n                SSS\\n                SSS\\n                SSS\\n                SSS\\n SSSSSSSSSSSS   SSS   SSSS       SSSS    SSSSSSSSS   SSSSSSSSSSSSSSSSSSSS\\nSSSSSSSSSSSSS   SSS   SSSS       SSSS   SSSSSSSSSS  SSSSSSSSSSSSSSSSSSSSSS\\nSSSS            SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\\nSSSS            SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\\nSSSSSSSSSSSS    SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\\n SSSSSSSSSSSS   SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\\n         SSSS   SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\\n         SSSS   SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\\nSSSSSSSSSSSSS   SSS   SSSSSSSSSSSSSSS   SSSS        SSSS     SSSS     SSSS\\nSSSSSSSSSSSS    SSS    SSSSSSSSSSSSS    SSSS        SSSS     SSSS     SSSS\\n\\n\\n\\'\\'\\'\\n\\ndef add_slurm_user():\\n\\n    SLURM_UID = str(992)\\n    subprocess.call([\\'groupadd\\', \\'-g\\', SLURM_UID, \\'slurm\\'])\\n    subprocess.call([\\'useradd\\', \\'-m\\', \\'-c\\', \\'SLURM Workload Manager\\',\\n        \\'-d\\', \\'/var/lib/slurm\\', \\'-u\\', SLURM_UID, \\'-g\\', \\'slurm\\',\\n        \\'-s\\', \\'/bin/bash\\', \\'slurm\\'])\\n\\n# END add_slurm_user()\\n\\n\\ndef start_motd():\\n\\n    msg = MOTD_HEADER + \"\"\"\\n*** Slurm is currently being installed/configured in the background. ***\\nA terminal broadcast will announce when installation and configuration is\\ncomplete.\\n\\n\"\"\"\\n\\n    if INSTANCE_TYPE != \"controller\":\\n        msg += \"\"\"/home on the controller will be mounted over the existing /home.\\nAny changes in /home will be hidden. Please wait until the installation is\\ncomplete before making changes in your home directory.\\n\\n\"\"\"\\n\\n    f = open(\\'/etc/motd\\', \\'w\\')\\n    f.write(msg)\\n    f.close()\\n\\n# END start_motd()\\n\\n\\ndef end_motd():\\n\\n    f = open(\\'/etc/motd\\', \\'w\\')\\n    f.write(MOTD_HEADER)\\n    f.close()\\n\\n    subprocess.call([\\'wall\\', \\'-n\\',\\n        \\'*** Slurm \\' + INSTANCE_TYPE + \\' daemon installation complete ***\\'])\\n\\n    if INSTANCE_TYPE != \"controller\":\\n        subprocess.call([\\'wall\\', \\'-n\\', \"\"\"\\n/home on the controller was mounted over the existing /home.\\nEither log out and log back in or cd into ~.\\n\"\"\"])\\n\\n#END start_motd()\\n\\n\\ndef have_internet():\\n    conn = httplib.HTTPConnection(\"www.google.com\", timeout=1)\\n    try:\\n        conn.request(\"HEAD\", \"/\")\\n        conn.close()\\n        return True\\n    except:\\n        conn.close()\\n        return False\\n\\n#END have_internet()\\n\\n\\ndef install_packages():\\n\\n    packages = [\\'bind-utils\\',\\n                \\'epel-release\\',\\n                \\'gcc\\',\\n                \\'hwloc\\',\\n                \\'hwloc-devel\\',\\n                \\'libibmad\\',\\n                \\'libibumad\\',\\n                \\'lua\\',\\n                \\'lua-devel\\',\\n                \\'man2html\\',\\n                \\'mariadb\\',\\n                \\'mariadb-devel\\',\\n                \\'mariadb-server\\',\\n                \\'munge\\',\\n                \\'munge-devel\\',\\n                \\'munge-libs\\',\\n                \\'ncurses-devel\\',\\n                \\'nfs-utils\\',\\n                \\'numactl\\',\\n                \\'numactl-devel\\',\\n                \\'openssl-devel\\',\\n                \\'pam-devel\\',\\n                \\'perl-ExtUtils-MakeMaker\\',\\n                \\'python-pip\\',\\n                \\'readline-devel\\',\\n                \\'rpm-build\\',\\n                \\'rrdtool-devel\\',\\n                \\'vim\\',\\n                \\'wget\\',\\n                \\'tmux\\',\\n                \\'pdsh\\',\\n                \\'openmpi\\'\\n               ]\\n\\n    while subprocess.call([\\'yum\\', \\'install\\', \\'-y\\'] + packages):\\n        print \"yum failed to install packages. Trying again in 5 seconds\"\\n        time.sleep(5)\\n\\n    while subprocess.call([\\'pip\\', \\'install\\', \\'--upgrade\\',\\n        \\'google-api-python-client\\']):\\n        print \"failed to install google python api client. Trying again 5 seconds.\"\\n        time.sleep(5)\\n\\n    if GPU_COUNT and (INSTANCE_TYPE == \"compute\"):\\n        rpm = \"cuda-repo-rhel7-10.0.130-1.x86_64.rpm\"\\n        subprocess.call(\"yum -y install kernel-devel-$(uname -r) kernel-headers-$(uname -r)\", shell=True)\\n        subprocess.call(shlex.split(\"wget http://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/\" + rpm))\\n        subprocess.call(shlex.split(\"rpm -i \" + rpm))\\n        subprocess.call(shlex.split(\"yum clean all\"))\\n        subprocess.call(shlex.split(\"yum -y install cuda\"))\\n        subprocess.call(shlex.split(\"nvidia-smi\")) # Creates the device files\\n\\n#END install_packages()\\n\\ndef setup_munge():\\n\\n    f = open(\\'/etc/fstab\\', \\'a\\')\\n    if not NFS_APPS_SERVER:\\n        if ((INSTANCE_TYPE != \"controller\")):\\n            f.write(\"\"\"\\n{1}:{0}    {0}     nfs      rw,sync,hard,intr  0     0\\n\"\"\".format(MUNGE_DIR, CONTROL_MACHINE))\\n    else:\\n        f.write(\"\"\"\\n{1}:{0}    {0}     nfs      rw,sync,hard,intr  0     0\\n\"\"\".format(MUNGE_DIR, NFS_APPS_SERVER))\\n    f.close()\\n\\n    if (INSTANCE_TYPE != \"controller\"):\\n        munge_over_path = \"/etc/systemd/system/munge.service.d\"\\n        if not os.path.exists(munge_over_path):\\n            os.makedirs(munge_over_path)\\n        f = open(munge_over_path + \"/override.conf\", \\'w\\')\\n        f.write(\"[Unit]\\\\nRequiresMountsFor={}\\\\n\".format(MUNGE_DIR))\\n        f.close()\\n\\n        return\\n\\n    if MUNGE_KEY:\\n        f = open(MUNGE_DIR +\\'/munge.key\\', \\'w\\')\\n        f.write(MUNGE_KEY)\\n        f.close()\\n\\n        subprocess.call([\\'chown\\', \\'-R\\', \\'munge:\\', MUNGE_DIR, \\'/var/log/munge/\\'])\\n        os.chmod(MUNGE_DIR + \\'/munge.key\\' ,0o400)\\n        os.chmod(MUNGE_DIR                ,0o700)\\n        os.chmod(\\'/var/log/munge/\\'        ,0o700)\\n    else:\\n        subprocess.call([\\'create-munge-key\\'])\\n\\n#END setup_munge ()\\n\\ndef start_munge():\\n        subprocess.call([\\'systemctl\\', \\'enable\\', \\'munge\\'])\\n        subprocess.call([\\'systemctl\\', \\'start\\', \\'munge\\'])\\n#END start_munge()\\n\\ndef setup_nfs_exports():\\n\\n    f = open(\\'/etc/exports\\', \\'w\\')\\n    f.write(\"\"\"\\n/home  *(rw,sync,no_subtree_check,no_root_squash)\\n%s  *(rw,sync,no_subtree_check,no_root_squash)\\n/etc/munge *(rw,sync,no_subtree_check,no_root_squash)\\n\"\"\" % APPS_DIR)\\n    if CONTROLLER_SECONDARY_DISK:\\n        f.write(\"\"\"\\n%s  *(rw,sync,no_subtree_check,no_root_squash)\\n\"\"\" % SEC_DISK_DIR)\\n    f.close()\\n\\n    subprocess.call(shlex.split(\"exportfs -a\"))\\n\\n#END setup_nfs_exports()\\n\\n\\ndef expand_machine_type():\\n\\n    # Force re-evaluation of site-packages so that namespace packages (such\\n    # as google-auth) are importable. This is needed because we install the\\n    # packages while this script is running and do not have the benefit of\\n    # restarting the interpreter for it to do it\\'s usual startup sequence to\\n    # configure import magic.\\n    import sys\\n    import site\\n    for path in [x for x in sys.path if \\'site-packages\\' in x]:\\n        site.addsitedir(path)\\n\\n    import googleapiclient.discovery\\n\\n    # Assume sockets is 1. Currently, no instances with multiple sockets\\n    # Assume hyper-threading is on and 2 threads per core\\n    machine = {\\'sockets\\': 1, \\'cores\\': 1, \\'threads\\': 1, \\'memory\\': 1}\\n\\n    try:\\n        compute = googleapiclient.discovery.build(\\'compute\\', \\'v1\\',\\n                                                  cache_discovery=False)\\n        type_resp = compute.machineTypes().get(project=PROJECT, zone=ZONE,\\n                machineType=MACHINE_TYPE).execute()\\n        if type_resp:\\n            tot_cpus = type_resp[\\'guestCpus\\']\\n            if tot_cpus > 1:\\n                machine[\\'cores\\']   = tot_cpus / 2\\n                machine[\\'threads\\'] = 2\\n\\n            # Because the actual memory on the host will be different than what\\n            # is configured (e.g. kernel will take it). From experiments, about\\n            # 16 MB per GB are used (plus about 400 MB buffer for the first\\n            # couple of GB\\'s. Using 30 MB to be safe.\\n            gb = type_resp[\\'memoryMb\\'] / 1024;\\n            machine[\\'memory\\'] = type_resp[\\'memoryMb\\'] - (400 + (gb * 30))\\n\\n    except Exception, e:\\n        print \"Failed to get MachineType \\'%s\\' from google api (%s)\" % (MACHINE_TYPE, str(e))\\n\\n    return machine\\n#END expand_machine_type()\\n\\n\\ndef install_slurm_conf():\\n\\n    machine = expand_machine_type()\\n    def_mem_per_cpu = max(100,\\n            (machine[\\'memory\\'] /\\n             (machine[\\'threads\\']*machine[\\'cores\\']*machine[\\'sockets\\'])))\\n\\n    conf = \"\"\"\\n# slurm.conf file generated by configurator.html.\\n# Put this file on all nodes of your cluster.\\n# See the slurm.conf man page for more information.\\n#\\nControlMachine={control_machine}\\n#ControlAddr=\\n#BackupController=\\n#BackupAddr=\\n#\\nAuthType=auth/munge\\n#CheckpointType=checkpoint/none\\nCryptoType=crypto/munge\\n#DisableRootJobs=NO\\n#EnforcePartLimits=NO\\n#Epilog=\\n#EpilogSlurmctld=\\n#FirstJobId=1\\n#MaxJobId=999999\\nGresTypes=gpu\\n#GroupUpdateForce=0\\n#GroupUpdateTime=600\\n#JobCheckpointDir=/var/slurm/checkpoint\\n#JobCredentialPrivateKey=\\n#JobCredentialPublicCertificate=\\n#JobFileAppend=0\\n#JobRequeue=1\\n#JobSubmitPlugins=1\\n#KillOnBadExit=0\\n#LaunchType=launch/slurm\\n#Licenses=foo*4,bar\\n#MailProg=/bin/mail\\n#MaxJobCount=5000\\n#MaxStepCount=40000\\n#MaxTasksPerNode=128\\nMpiDefault=none\\n#MpiParams=ports=#-#\\n#PluginDir=\\n#PlugStackConfig=\\n#PrivateData=jobs\\n\\n# Always show cloud nodes. Otherwise cloud nodes are hidden until they are\\n# resumed. Having them shown can be useful in detecting downed nodes.\\n# NOTE: slurm won\\'t allocate/resume nodes that are down. So in the case of\\n# preemptible nodes -- if gcp preempts a node, the node will eventually be put\\n# into a down date because the node will stop responding to the controller.\\n# (e.g. SlurmdTimeout).\\nPrivateData=cloud\\n\\nProctrackType=proctrack/cgroup\\n\\n#Prolog=\\n#PrologFlags=\\n#PrologSlurmctld=\\n#PropagatePrioProcess=0\\n#PropagateResourceLimits=\\n#PropagateResourceLimitsExcept=Sched\\n#RebootProgram=\\n\\nReturnToService=2\\n#SallocDefaultCommand=\\nSlurmctldPidFile=/var/run/slurm/slurmctld.pid\\nSlurmctldPort=6817\\nSlurmdPidFile=/var/run/slurm/slurmd.pid\\nSlurmdPort=6818\\nSlurmdSpoolDir=/var/spool/slurmd\\nSlurmUser=slurm\\n#SlurmdUser=root\\n#SrunEpilog=\\n#SrunProlog=\\nStateSaveLocation={apps_dir}/slurm/state\\nSwitchType=switch/none\\n#TaskEpilog=\\nTaskPlugin=task/affinity,task/cgroup\\n#TaskPluginParam=\\n#TaskProlog=\\n#TopologyPlugin=topology/tree\\n#TmpFS=/tmp\\n#TrackWCKey=no\\n#TreeWidth=\\n#UnkillableStepProgram=\\n#UsePAM=0\\n#\\n#\\n# TIMERS\\n#BatchStartTimeout=10\\n#CompleteWait=0\\n#EpilogMsgTime=2000\\n#GetEnvTimeout=2\\n#HealthCheckInterval=0\\n#HealthCheckProgram=\\nInactiveLimit=0\\nKillWait=30\\n#MessageTimeout=10\\n#ResvOverRun=0\\nMinJobAge=300\\n#OverTimeLimit=0\\nSlurmctldTimeout=120\\nSlurmdTimeout=300\\n#UnkillableStepTimeout=60\\n#VSizeFactor=0\\nWaittime=0\\n#\\n#\\n# SCHEDULING\\nFastSchedule=1\\nDefMemPerCPU={def_mem_per_cpu}\\n#MaxMemPerCPU=0\\n#SchedulerTimeSlice=30\\nSchedulerType=sched/backfill\\nSelectType=select/cons_res\\nSelectTypeParameters=CR_Core_Memory\\n#\\n#\\n# JOB PRIORITY\\n#PriorityFlags=\\n#PriorityType=priority/basic\\n#PriorityDecayHalfLife=\\n#PriorityCalcPeriod=\\n#PriorityFavorSmall=\\n#PriorityMaxAge=\\n#PriorityUsageResetPeriod=\\n#PriorityWeightAge=\\n#PriorityWeightFairshare=\\n#PriorityWeightJobSize=\\n#PriorityWeightPartition=\\n#PriorityWeightQOS=\\n#\\n#\\n# LOGGING AND ACCOUNTING\\nAccountingStorageEnforce=associations,limits,qos,safe\\nAccountingStorageHost={control_machine}\\n#AccountingStorageLoc=\\n#AccountingStoragePass=\\n#AccountingStoragePort=\\nAccountingStorageType=accounting_storage/slurmdbd\\n#AccountingStorageUser=\\nAccountingStoreJobComment=YES\\nClusterName={cluster_name}\\nDebugFlags=power\\n#JobCompHost=\\n#JobCompLoc=\\n#JobCompPass=\\n#JobCompPort=\\nJobCompType=jobcomp/none\\n#JobCompUser=\\n#JobContainerType=job_container/none\\nJobAcctGatherFrequency=30\\nJobAcctGatherType=jobacct_gather/linux\\nSlurmctldDebug=info\\nSlurmctldLogFile={apps_dir}/slurm/log/slurmctld.log\\nSlurmdDebug=debug\\nSlurmdLogFile=/var/log/slurm/slurmd-%n.log\\n#\\n#\\n# POWER SAVE SUPPORT FOR IDLE NODES (optional)\\nSuspendProgram={apps_dir}/slurm/scripts/suspend.py\\nResumeProgram={apps_dir}/slurm/scripts/resume.py\\nSuspendTimeout=300\\nResumeTimeout=1800\\n#ResumeRate=\\n#SuspendExcNodes=\\n#SuspendExcParts=\\nSuspendRate=0\\nSuspendTime=2100\\n#\\n#\\n# COMPUTE NODES\\n\"\"\".format(apps_dir        = APPS_DIR,\\n           cluster_name    = CLUSTER_NAME,\\n           control_machine = CONTROL_MACHINE,\\n           def_mem_per_cpu = def_mem_per_cpu)\\n\\n    conf += \\' \\'.join((\"NodeName=DEFAULT\",\\n                      \"Sockets=\"        + str(machine[\\'sockets\\']),\\n                      \"CoresPerSocket=\" + str(machine[\\'cores\\']),\\n                      \"ThreadsPerCore=\" + str(machine[\\'threads\\']),\\n                      \"RealMemory=\"     + str(machine[\\'memory\\']),\\n                      \"State=UNKNOWN\"))\\n\\n    if GPU_COUNT:\\n        conf += \" Gres=gpu:\" + str(GPU_COUNT)\\n    conf += \"\\\\n\"\\n\\n    static_range = \"\"\\n    if STATIC_NODE_COUNT and STATIC_NODE_COUNT > 1:\\n        static_range = \"[1-%d]\" % STATIC_NODE_COUNT\\n    elif STATIC_NODE_COUNT:\\n        static_range = \"1\"\\n\\n    cloud_range = \"\"\\n    if MAX_NODE_COUNT and (MAX_NODE_COUNT != STATIC_NODE_COUNT):\\n        cloud_range = \"[%d-%d]\" % (STATIC_NODE_COUNT+1, MAX_NODE_COUNT)\\n\\n    if static_range:\\n        conf += \"\"\"\\nSuspendExcNodes={1}-compute{0}\\nNodeName={1}-compute{0}\\n\"\"\".format(static_range, CLUSTER_NAME)\\n\\n    if cloud_range:\\n        conf += \"NodeName={0}-compute{1} State=CLOUD\".format(CLUSTER_NAME, cloud_range)\\n\\n    conf += \"\"\"\\nPartitionName=debug Nodes={0}-compute[1-{1:d}] Default=YES MaxTime=INFINITE State=UP LLN=yes\\n\"\"\".format(CLUSTER_NAME, MAX_NODE_COUNT)\\n\\n    etc_dir = SLURM_PREFIX + \\'/etc\\'\\n    if not os.path.exists(etc_dir):\\n        os.makedirs(etc_dir)\\n    f = open(etc_dir + \\'/slurm.conf\\', \\'w\\')\\n    f.write(conf)\\n    f.close()\\n#END install_slurm_conf()\\n\\n\\ndef install_slurmdbd_conf():\\n\\n    conf = \"\"\"\\n#ArchiveEvents=yes\\n#ArchiveJobs=yes\\n#ArchiveResvs=yes\\n#ArchiveSteps=no\\n#ArchiveSuspend=no\\n#ArchiveTXN=no\\n#ArchiveUsage=no\\n\\nAuthType=auth/munge\\nDbdHost={control_machine}\\nDebugLevel=debug2\\n\\n#PurgeEventAfter=1month\\n#PurgeJobAfter=12month\\n#PurgeResvAfter=1month\\n#PurgeStepAfter=1month\\n#PurgeSuspendAfter=1month\\n#PurgeTXNAfter=12month\\n#PurgeUsageAfter=24month\\n\\nLogFile={apps_dir}/slurm/log/slurmdbd.log\\nPidFile=/var/run/slurm/slurmdbd.pid\\n\\nSlurmUser=slurm\\nStorageUser=slurm\\n\\nStorageLoc=slurm_acct_db\\n\\nStorageType=accounting_storage/mysql\\n#StorageUser=database_mgr\\n#StoragePass=shazaam\\n\\n\"\"\".format(apps_dir = APPS_DIR, control_machine = CONTROL_MACHINE)\\n    etc_dir = SLURM_PREFIX + \\'/etc\\'\\n    if not os.path.exists(etc_dir):\\n        os.makedirs(etc_dir)\\n    f = open(etc_dir + \\'/slurmdbd.conf\\', \\'w\\')\\n    f.write(conf)\\n    f.close()\\n\\n#END install_slurmdbd_conf()\\n\\n\\ndef install_cgroup_conf():\\n\\n    conf = \"\"\"\\nCgroupAutomount=no\\n#CgroupMountpoint=/sys/fs/cgroup\\nConstrainCores=yes\\nConstrainRamSpace=yes\\nConstrainSwapSpace=yes\\nTaskAffinity=no\\nConstrainDevices=yes\\n\"\"\"\\n\\n    etc_dir = SLURM_PREFIX + \\'/etc\\'\\n    f = open(etc_dir + \\'/cgroup.conf\\', \\'w\\')\\n    f.write(conf)\\n    f.close()\\n\\n    f = open(etc_dir + \\'/cgroup_allowed_devices_file.conf\\', \\'w\\')\\n    f.write(\"\")\\n    f.close()\\n\\n    if GPU_COUNT:\\n        f = open(etc_dir + \\'/gres.conf\\', \\'w\\')\\n        f.write(\"NodeName=%s-compute[1-%d] Name=gpu File=/dev/nvidia[0-%d]\"\\n                % (CLUSTER_NAME, MAX_NODE_COUNT, (GPU_COUNT - 1)))\\n        f.close()\\n#END install_cgroup_conf()\\n\\n\\ndef install_suspend_progs():\\n\\n    if not os.path.exists(APPS_DIR + \\'/slurm/scripts\\'):\\n        os.makedirs(APPS_DIR + \\'/slurm/scripts\\')\\n\\n    GOOGLE_URL = \"http://metadata.google.internal/computeMetadata/v1/instance/attributes\"\\n\\n    # Suspend\\n    req = urllib2.Request(GOOGLE_URL + \\'/slurm_suspend\\')\\n    req.add_header(\\'Metadata-Flavor\\', \\'Google\\')\\n    resp = urllib2.urlopen(req)\\n\\n    f = open(APPS_DIR + \\'/slurm/scripts/suspend.py\\', \\'w\\')\\n    f.write(resp.read())\\n    f.close()\\n    os.chmod(APPS_DIR + \\'/slurm/scripts/suspend.py\\', 0o755)\\n\\n    # Resume\\n    req = urllib2.Request(GOOGLE_URL + \\'/slurm_resume\\')\\n    req.add_header(\\'Metadata-Flavor\\', \\'Google\\')\\n    resp = urllib2.urlopen(req)\\n\\n    f = open(APPS_DIR + \\'/slurm/scripts/resume.py\\', \\'w\\')\\n    f.write(resp.read())\\n    f.close()\\n    os.chmod(APPS_DIR + \\'/slurm/scripts/resume.py\\', 0o755)\\n\\n    # Startup script\\n    req = urllib2.Request(GOOGLE_URL + \\'/startup-script-compute\\')\\n    req.add_header(\\'Metadata-Flavor\\', \\'Google\\')\\n    resp = urllib2.urlopen(req)\\n\\n    f = open(APPS_DIR + \\'/slurm/scripts/startup-script.py\\', \\'w\\')\\n    f.write(resp.read())\\n    f.close()\\n    os.chmod(APPS_DIR + \\'/slurm/scripts/startup-script.py\\', 0o755)\\n\\n#END install_suspend_progs()\\n\\ndef install_slurm():\\n\\n    SCHEDMD_URL = \\'https://download.schedmd.com/slurm/\\'\\n    file = \"slurm-%s.tar.bz2\" % SLURM_VERSION\\n    urllib.urlretrieve(SCHEDMD_URL + file, \\'/tmp/\\' + file)\\n\\n    prev_path = os.getcwd()\\n\\n    os.chdir(\\'/tmp\\')\\n    subprocess.call([\\'tar\\', \\'-xvjf\\', file])\\n    os.chdir(\\'/tmp/slurm-\\' + SLURM_VERSION)\\n    if not os.path.exists(\\'build\\'):\\n        os.makedirs(\\'build\\')\\n    os.chdir(\\'build\\')\\n    subprocess.call([\\'../configure\\', \\'--prefix=%s\\' % SLURM_PREFIX,\\n                     \\'--sysconfdir=%s/slurm/current/etc\\' % APPS_DIR])\\n    subprocess.call([\\'make\\', \\'-j\\', \\'install\\'])\\n\\n    subprocess.call(shlex.split(\"ln -s %s %s/slurm/current\" % (SLURM_PREFIX, APPS_DIR)))\\n\\n    os.chdir(prev_path)\\n\\n    if not os.path.exists(APPS_DIR + \\'/slurm/state\\'):\\n        os.makedirs(APPS_DIR + \\'/slurm/state\\')\\n        subprocess.call([\\'chown\\', \\'-R\\', \\'slurm:\\', APPS_DIR + \\'/slurm/state\\'])\\n    if not os.path.exists(APPS_DIR + \\'/slurm/log\\'):\\n        os.makedirs(APPS_DIR + \\'/slurm/log\\')\\n        subprocess.call([\\'chown\\', \\'-R\\', \\'slurm:\\', APPS_DIR + \\'/slurm/log\\'])\\n\\n    install_slurm_conf()\\n    install_slurmdbd_conf()\\n    install_cgroup_conf()\\n    install_suspend_progs()\\n\\n#END install_slurm()\\n\\ndef install_slurm_tmpfile():\\n\\n    run_dir = \\'/var/run/slurm\\'\\n\\n    f = open(\\'/etc/tmpfiles.d/slurm.conf\\', \\'w\\')\\n    f.write(\"\"\"\\nd %s 0755 slurm slurm -\\n\"\"\" % run_dir)\\n    f.close()\\n\\n    if not os.path.exists(run_dir):\\n        os.makedirs(run_dir)\\n\\n    os.chmod(run_dir, 0o755)\\n    subprocess.call([\\'chown\\', \\'slurm:\\', run_dir])\\n\\n#END install_slurm_tmpfile()\\n\\ndef install_controller_service_scripts():\\n\\n    install_slurm_tmpfile()\\n\\n    # slurmctld.service\\n    f = open(\\'/usr/lib/systemd/system/slurmctld.service\\', \\'w\\')\\n    f.write(\"\"\"\\n[Unit]\\nDescription=Slurm controller daemon\\nAfter=network.target munge.service\\nConditionPathExists={prefix}/etc/slurm.conf\\n\\n[Service]\\nType=forking\\nEnvironmentFile=-/etc/sysconfig/slurmctld\\nExecStart={prefix}/sbin/slurmctld $SLURMCTLD_OPTIONS\\nExecReload=/bin/kill -HUP $MAINPID\\nPIDFile=/var/run/slurm/slurmctld.pid\\n\\n[Install]\\nWantedBy=multi-user.target\\n\"\"\".format(prefix = APPS_DIR + \"/slurm/current\"))\\n    f.close()\\n\\n    os.chmod(\\'/usr/lib/systemd/system/slurmctld.service\\', 0o644)\\n\\n    # slurmdbd.service\\n    f = open(\\'/usr/lib/systemd/system/slurmdbd.service\\', \\'w\\')\\n    f.write(\"\"\"\\n[Unit]\\nDescription=Slurm DBD accounting daemon\\nAfter=network.target munge.service\\nConditionPathExists={prefix}/etc/slurmdbd.conf\\n\\n[Service]\\nType=forking\\nEnvironmentFile=-/etc/sysconfig/slurmdbd\\nExecStart={prefix}/sbin/slurmdbd $SLURMDBD_OPTIONS\\nExecReload=/bin/kill -HUP $MAINPID\\nPIDFile=/var/run/slurm/slurmdbd.pid\\n\\n[Install]\\nWantedBy=multi-user.target\\n\"\"\".format(prefix = APPS_DIR + \"/slurm/current\"))\\n    f.close()\\n\\n    os.chmod(\\'/usr/lib/systemd/system/slurmdbd.service\\', 0o644)\\n\\n#END install_controller_service_scripts()\\n\\n\\ndef install_compute_service_scripts():\\n\\n    install_slurm_tmpfile()\\n\\n    # slurmd.service\\n    f = open(\\'/usr/lib/systemd/system/slurmd.service\\', \\'w\\')\\n    f.write(\"\"\"\\n[Unit]\\nDescription=Slurm node daemon\\nAfter=network.target munge.service\\nConditionPathExists={prefix}/etc/slurm.conf\\n\\n[Service]\\nType=forking\\nEnvironmentFile=-/etc/sysconfig/slurmd\\nExecStart={prefix}/sbin/slurmd $SLURMD_OPTIONS\\nExecReload=/bin/kill -HUP $MAINPID\\nPIDFile=/var/run/slurm/slurmd.pid\\nKillMode=process\\nLimitNOFILE=51200\\nLimitMEMLOCK=infinity\\nLimitSTACK=infinity\\n\\n[Install]\\nWantedBy=multi-user.target\\n\"\"\".format(prefix = APPS_DIR + \"/slurm/current\"))\\n    f.close()\\n\\n    os.chmod(\\'/usr/lib/systemd/system/slurmd.service\\', 0o644)\\n\\n#END install_compute_service_scripts()\\n\\n\\ndef setup_bash_profile():\\n\\n    f = open(\\'/etc/profile.d/slurm.sh\\', \\'w\\')\\n    f.write(\"\"\"\\nS_PATH=%s/slurm/current\\nPATH=$PATH:$S_PATH/bin:$S_PATH/sbin\\n\"\"\" % APPS_DIR)\\n    f.close()\\n\\n    if GPU_COUNT and (INSTANCE_TYPE == \"compute\"):\\n        f = open(\\'/etc/profile.d/cuda.sh\\', \\'w\\')\\n        f.write(\"\"\"\\nCUDA_PATH=/usr/local/cuda\\nPATH=$CUDA_PATH/bin${PATH:+:${PATH}}\\nLD_LIBRARY_PATH=$CUDA_PATH/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\\n\"\"\")\\n        f.close()\\n\\n#END setup_bash_profile()\\n\\ndef setup_nfs_apps_vols():\\n\\n    f = open(\\'/etc/fstab\\', \\'a\\')\\n    if not NFS_APPS_SERVER:\\n        if ((INSTANCE_TYPE != \"controller\")):\\n            f.write(\"\"\"\\n{1}:{0}    {0}     nfs      rw,sync,hard,intr  0     0\\n\"\"\".format(APPS_DIR, CONTROL_MACHINE))\\n    else:\\n        f.write(\"\"\"\\n{1}:{0}    {0}     nfs      rw,sync,hard,intr  0     0\\n\"\"\".format(APPS_DIR, NFS_APPS_SERVER))\\n    f.close()\\n\\n#END setup_nfs_apps_vols()\\n\\ndef setup_nfs_home_vols():\\n\\n    f = open(\\'/etc/fstab\\', \\'a\\')\\n    if not NFS_HOME_SERVER:\\n        if ((INSTANCE_TYPE != \"controller\")):\\n            f.write(\"\"\"\\n{0}:/home    /home     nfs      rw,sync,hard,intr  0     0\\n\"\"\".format(CONTROL_MACHINE))\\n    else:\\n        f.write(\"\"\"\\n{0}:/home    /home     nfs      rw,sync,hard,intr  0     0\\n\"\"\".format(NFS_HOME_SERVER))\\n    f.close()\\n\\n#END setup_nfs_home_vols()\\n\\ndef setup_nfs_sec_vols():\\n    f = open(\\'/etc/fstab\\', \\'a\\')\\n\\n    if CONTROLLER_SECONDARY_DISK:\\n        if ((INSTANCE_TYPE != \"controller\")):\\n            f.write(\"\"\"\\n{1}:{0}    {0}     nfs      rw,sync,hard,intr  0     0\\n\"\"\".format(SEC_DISK_DIR, CONTROL_MACHINE))\\n    f.close()\\n\\n#END setup_nfs_sec_vols()\\n\\ndef setup_secondary_disks():\\n\\n    subprocess.call(shlex.split(\"sudo mkfs.ext4 -m 0 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/sdb\"))\\n    f = open(\\'/etc/fstab\\', \\'a\\')\\n\\n    f.write(\"\"\"\\n/dev/sdb    {0}  ext4    discard,defaults,nofail  0  2\\n\"\"\".format(SEC_DISK_DIR))\\n    f.close()\\n\\n#END setup_secondary_disks()\\n\\ndef mount_nfs_vols():\\n    while subprocess.call([\\'mount\\', \\'-a\\']):\\n        print \"Waiting for \" + APPS_DIR + \" and /home to be mounted\"\\n        time.sleep(5)\\n\\n#END mount_nfs_vols()\\n\\n# Tune the NFS server to support many mounts\\ndef setup_nfs_threads():\\n\\n    f = open(\\'/etc/sysconfig/nfs\\', \\'a\\')\\n    f.write(\"\"\"\\n# Added by Google\\nRPCNFSDCOUNT=256\\n\"\"\".format(APPS_DIR))\\n    f.close()\\n\\n# END setup_nfs_threads()\\n\\ndef setup_slurmd_cronjob():\\n    #subprocess.call(shlex.split(\\'crontab < /apps/slurm/scripts/cron\\'))\\n    os.system(\"echo \\'*/2 * * * * if [ `systemctl status slurmd | grep -c inactive` -gt 0 ]; then mount -a; systemctl restart slurmd; fi\\' | crontab -u root -\")\\n# END setup_slurmd_cronjob()\\n\\ndef format_disk():\\n    subprocess.call(shlex.split(\"sudo mkfs.ext4 -m 0 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/sdb\"))\\n    #subprocess.call(shlex.split(\"sudo mkdir -p \" + SEC_DISK_DIR))\\n    subprocess.call(shlex.split(\"sudo mount -o discard,defaults /dev/sdb \" + SEC_DISK_DIR))\\n    subprocess.call(shlex.split(\"sudo chmod a+w \" + SEC_DISK_DIR))\\n\\n# END format_disk()\\n\\ndef main():\\n    # Disable SELinux\\n    subprocess.call(shlex.split(\\'setenforce 0\\'))\\n\\n    if INSTANCE_TYPE == \"compute\":\\n        while not have_internet():\\n            print \"Waiting for internet connection\"\\n\\n    if not os.path.exists(APPS_DIR + \\'/slurm\\'):\\n        os.makedirs(APPS_DIR + \\'/slurm\\')\\n        print \"ww Created Slurm Folders\"\\n\\n    if CONTROLLER_SECONDARY_DISK:\\n        if not os.path.exists(SEC_DISK_DIR):\\n            os.makedirs(SEC_DISK_DIR)\\n\\n    start_motd()\\n\\n    if not os.path.exists(\\'/var/log/slurm\\'):\\n        os.makedirs(\\'/var/log/slurm\\')\\n\\n    add_slurm_user()\\n    install_packages()\\n    setup_munge()\\n    setup_bash_profile()\\n\\n    if (CONTROLLER_SECONDARY_DISK and (INSTANCE_TYPE == \"controller\")):\\n        setup_secondary_disks()\\n\\n    setup_nfs_apps_vols()\\n    setup_nfs_home_vols()\\n    setup_nfs_sec_vols()\\n    mount_nfs_vols()\\n\\n    start_munge()\\n\\n    if INSTANCE_TYPE == \"controller\":\\n\\n        install_slurm()\\n\\n        # Add any additional installation functions here\\n\\n        install_controller_service_scripts()\\n\\n        subprocess.call(shlex.split(\\'systemctl enable mariadb\\'))\\n        subprocess.call(shlex.split(\\'systemctl start mariadb\\'))\\n\\n        subprocess.call([\\'mysql\\', \\'-u\\', \\'root\\', \\'-e\\',\\n            \"create user \\'slurm\\'@\\'localhost\\'\"])\\n        subprocess.call([\\'mysql\\', \\'-u\\', \\'root\\', \\'-e\\',\\n            \"grant all on slurm_acct_db.* TO \\'slurm\\'@\\'localhost\\';\"])\\n        subprocess.call([\\'mysql\\', \\'-u\\', \\'root\\', \\'-e\\',\\n            \"grant all on slurm_acct_db.* TO \\'slurm\\'@\\'{0}\\';\".format(CONTROL_MACHINE)])\\n\\n        subprocess.call(shlex.split(\\'systemctl enable slurmdbd\\'))\\n        subprocess.call(shlex.split(\\'systemctl start slurmdbd\\'))\\n\\n        # Wait for slurmdbd to come up\\n        time.sleep(5)\\n\\n        oslogin_chars = [\\'@\\', \\'.\\']\\n\\n        SLURM_USERS = DEF_SLURM_USERS\\n\\n        for char in oslogin_chars:\\n            SLURM_USERS = SLURM_USERS.replace(char, \\'_\\')\\n\\n        subprocess.call(shlex.split(SLURM_PREFIX + \\'/bin/sacctmgr -i add cluster \\' + CLUSTER_NAME))\\n        subprocess.call(shlex.split(SLURM_PREFIX + \\'/bin/sacctmgr -i add account \\' + DEF_SLURM_ACCT))\\n        subprocess.call(shlex.split(SLURM_PREFIX + \\'/bin/sacctmgr -i add user \\' + SLURM_USERS + \\' account=\\' + DEF_SLURM_ACCT))\\n\\n        subprocess.call(shlex.split(\\'systemctl enable slurmctld\\'))\\n        subprocess.call(shlex.split(\\'systemctl start slurmctld\\'))\\n        setup_nfs_threads()\\n        # Export at the end to signal that everything is up\\n        subprocess.call(shlex.split(\\'systemctl enable nfs-server\\'))\\n        subprocess.call(shlex.split(\\'systemctl start nfs-server\\'))\\n        setup_nfs_exports()\\n        print \"ww Done installing controller\"\\n        subprocess.call(shlex.split(\\'gcloud compute instances remove-metadata \\'+ CONTROL_MACHINE + \\' --zone=\\' + ZONE + \\' --keys=startup-script\\'))\\n\\n    elif INSTANCE_TYPE == \"compute\":\\n        install_compute_service_scripts()\\n\\n        hostname = socket.gethostname()\\n\\n        # Add any additional installation functions here\\n\\n        subprocess.call(shlex.split(\\'systemctl enable slurmd\\'))\\n        setup_slurmd_cronjob()\\n        subprocess.call(shlex.split(\\'systemctl start slurmd\\'))\\n        subprocess.call(shlex.split(\\'gcloud compute instances remove-metadata \\'+ hostname + \\' --zone=\\' + ZONE + \\' --keys=startup-script\\'))\\n\\n    end_motd()\\n\\n# END main()\\n\\n\\nif __name__ == \\'__main__\\':\\n    main()\\n'}],\n",
       "    u'kind': u'compute#metadata'},\n",
       "   u'name': u'sl1-controller',\n",
       "   u'networkInterfaces': [{u'accessConfigs': [{u'kind': u'compute#accessConfig',\n",
       "       u'name': u'External NAT',\n",
       "       u'natIP': u'35.193.188.103',\n",
       "       u'networkTier': u'PREMIUM',\n",
       "       u'type': u'ONE_TO_ONE_NAT'}],\n",
       "     u'fingerprint': u'Qhq4_NQfuh0=',\n",
       "     u'kind': u'compute#networkInterface',\n",
       "     u'name': u'nic0',\n",
       "     u'network': u'https://www.googleapis.com/compute/v1/projects/first-220321/global/networks/sl1-slurm-network',\n",
       "     u'networkIP': u'10.10.0.4',\n",
       "     u'subnetwork': u'https://www.googleapis.com/compute/v1/projects/first-220321/regions/us-central1/subnetworks/sl1-slurm-subnet'}],\n",
       "   u'scheduling': {u'automaticRestart': True,\n",
       "    u'onHostMaintenance': u'MIGRATE',\n",
       "    u'preemptible': False},\n",
       "   u'selfLink': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/instances/sl1-controller',\n",
       "   u'serviceAccounts': [{u'email': u'316905473554-compute@developer.gserviceaccount.com',\n",
       "     u'scopes': [u'https://www.googleapis.com/auth/cloud-platform']}],\n",
       "   u'startRestricted': False,\n",
       "   u'status': u'RUNNING',\n",
       "   u'tags': {u'fingerprint': u'e2aTiSrlOyM=', u'items': [u'controller']},\n",
       "   u'zone': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f'}],\n",
       " u'kind': u'compute#instanceList',\n",
       " u'selfLink': u'https://www.googleapis.com/compute/v1/projects/first-220321/zones/us-central1-f/instances'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute.instances().list(project=project,zone=ze).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_instance(mtype, zone, name=None):\n",
    "    global wordlist \n",
    "    global vmnum \n",
    "    \n",
    "    vmnum = vmnum+1 \n",
    "    \n",
    "    if name is None:\n",
    "        name = \"bravo\"+str(vmnum)\n",
    "    \n",
    "    machine_type = \"zones/{}/machineTypes/{}\".format(zone, mtype)\n",
    "    \n",
    "    \n",
    "    instance_body={\n",
    "        'name':name,\n",
    "        'machineType':machine_type, \n",
    "        'scheduling':\n",
    "          {\n",
    "        'preemptible': 'true'\n",
    "          },\n",
    "        'disks': [\n",
    "        {\n",
    "            'boot': True,\n",
    "            'autoDelete': True,\n",
    "            'initializeParams': {\n",
    "                'sourceImage': \"global/images/ubs2\"\n",
    "                }\n",
    "        }],\n",
    "        'metadata' : {\n",
    "            \"items\" : [\n",
    "                {\n",
    "                    \"key\": \"startup-script\",\n",
    "                    \"value\":startupscriptstr\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        'networkInterfaces': [{\n",
    "        'network': 'global/networks/default',\n",
    "        'accessConfigs': [\n",
    "            {'type': 'ONE_TO_ONE_NAT', 'name': 'External NAT'}\n",
    "        ]\n",
    "    }],\n",
    "    }\n",
    "    print(str(instance_body))\n",
    "    \n",
    "    response = compute.instances().insert(project=project,zone=zone,body=instance_body).execute()\n",
    "    return response \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtype='n1-standard-2'\n",
    "zone=ze \n",
    "name=\"bravo4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val': '#!/bin/bash\\n\\nlogger \"Startup Script Begins.... \"\\nlogger \"Running as `whoami`\" \\n\\n#systemctl stop slurmd \\n\\ncat <<\\\\EOF >> /etc/slurm-llnl/slurm.conf\\n\\n# slurm.conf file generated by configurator easy.html.\\n# Put this file on all nodes of your cluster.\\n# See the slurm.conf man page for more information.\\n#\\nControlMachine=slmaster\\nAuthType=auth/none\\n#CheckpointType=checkpoint/none\\n#CryptoType=crypto/munge\\n#ControlAddr=\\n#\\n#MailProg=/bin/mail\\nMpiDefault=none\\n#MpiParams=ports=#-#\\nProctrackType=proctrack/pgid\\nReturnToService=1\\nSlurmctldPidFile=/var/run/slurm-llnl/slurmctld.pid\\n#SlurmctldPort=6817\\nSlurmdPidFile=/var/run/slurm-llnl/slurmd.pid\\n#SlurmdPort=6818\\nSlurmdSpoolDir=/var/lib/slurm-llnl/slurmd\\nSlurmUser=slurm\\n#SlurmdUser=root\\nStateSaveLocation=/var/lib/slurm-llnl/slurmctld\\nSwitchType=switch/none\\nTaskPlugin=task/none\\nJobCredentialPrivateKey=/home/prateek3_14/slurmkey\\n#\\n#\\n# TIMERS\\n#KillWait=30\\n#MinJobAge=300\\n#SlurmctldTimeout=120\\n#SlurmdTimeout=300\\n#\\n#\\n# SCHEDULING\\nFastSchedule=1\\nSchedulerType=sched/builtin\\n#SchedulerPort=7321\\nSelectType=select/linear\\n#\\n#\\n# LOGGING AND ACCOUNTING\\nAccountingStorageType=accounting_storage/none\\nClusterName=ubslurm1\\n#JobAcctGatherFrequency=30\\nJobAcctGatherType=jobacct_gather/none\\n#SlurmctldDebug=3\\nSlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log\\n#SlurmdDebug=3\\nSlurmdLogFile=/var/log/slurm-llnl/slurmd.log\\n#\\n#\\n# COMPUTE NODES\\nNodeName=DEFAULT Sockets=1 CoresPerSocket=4 ThreadsPerCore=2 State=UNKNOWN\\n PartitionName=long Nodes=bravo[1-10] Default=YES MaxTime=INFINITE State=UP \\n\\nEOF\\n\\nsystemctl start slurmd \\n\\nlogger \"Slurm conf applied, startup script ending\" \\n\\nexit 0\\n\\n'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'machineType': 'zones/us-east1-b/machineTypes/n1-standard-2', 'name': 'bravo4', 'disks': [{'initializeParams': {'sourceImage': 'global/images/ubs2'}, 'boot': True, 'autoDelete': True}], 'scheduling': {'preemptible': 'true'}, 'metadata': {'items': [{'value': '#!/bin/bash\\n\\nlogger \"Startup Script Begins.... \"\\nlogger \"Running as `whoami`\" \\n\\n#systemctl stop slurmd \\n\\ncat <<\\\\EOF >> /etc/slurm-llnl/slurm.conf\\n\\n# slurm.conf file generated by configurator easy.html.\\n# Put this file on all nodes of your cluster.\\n# See the slurm.conf man page for more information.\\n#\\nControlMachine=ubslurm1\\nAuthType=auth/none\\n#CheckpointType=checkpoint/none\\n#CryptoType=crypto/munge\\n#ControlAddr=\\n#\\n#MailProg=/bin/mail\\nMpiDefault=none\\n#MpiParams=ports=#-#\\nProctrackType=proctrack/pgid\\nReturnToService=1\\nSlurmctldPidFile=/var/run/slurm-llnl/slurmctld.pid\\n#SlurmctldPort=6817\\nSlurmdPidFile=/var/run/slurm-llnl/slurmd.pid\\n#SlurmdPort=6818\\nSlurmdSpoolDir=/var/lib/slurm-llnl/slurmd\\nSlurmUser=slurm\\n#SlurmdUser=root\\nStateSaveLocation=/var/lib/slurm-llnl/slurmctld\\nSwitchType=switch/none\\nTaskPlugin=task/none\\nJobCredentialPrivateKey=/home/prateek3_14/slurmkey\\n#\\n#\\n# TIMERS\\n#KillWait=30\\n#MinJobAge=300\\n#SlurmctldTimeout=120\\n#SlurmdTimeout=300\\n#\\n#\\n# SCHEDULING\\nFastSchedule=1\\nSchedulerType=sched/builtin\\n#SchedulerPort=7321\\nSelectType=select/linear\\n#\\n#\\n# LOGGING AND ACCOUNTING\\nAccountingStorageType=accounting_storage/none\\nClusterName=ubslurm1\\n#JobAcctGatherFrequency=30\\nJobAcctGatherType=jobacct_gather/none\\n#SlurmctldDebug=3\\nSlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log\\n#SlurmdDebug=3\\nSlurmdLogFile=/var/log/slurm-llnl/slurmd.log\\n#\\n#\\n# COMPUTE NODES\\nNodeName=DEFAULT Sockets=1 CoresPerSocket=1 ThreadsPerCore=2 State=UNKNOWN\\nNodeName=ubslurm1 \\nNodeName=bravo[1-10] \\n\\n PartitionName=long Nodes=bravo[1-10] Default=YES MaxTime=INFINITE State=UP \\n\\nEOF\\n\\nsystemctl start slurmd \\n\\nlogger \"Slurm conf applied, startup script ending\" \\n\\nexit 0\\n\\n', 'key': 'startup-script'}]}, 'networkInterfaces': [{'accessConfigs': [{'type': 'ONE_TO_ONE_NAT', 'name': 'External NAT'}], 'network': 'global/networks/default'}]}\n"
     ]
    }
   ],
   "source": [
    "response = start_instance(mtype,ze,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
