
The schedmd yaml works. 

Some warning about disk size (100GB vs 10)

run slurm commands as root, for whatever reason 
(=sacctmgr list accounts/users=)

=srun -N 2 hostname=

=sinfo -a=
idle~ means in power-save mode 

stopping moves the node to idle* state, which means unreachable. 

srun -N 2 hostname with only one worker hangs? 

** Sbatch

sbatch --parsable 

strigger scripts on node failure, job completion 

Still need some kind of listener thread for the "signals". Simplest is just the basic twisted http listener. On job completion etc, machine failure, or other callbacks. 

also need sacct argh. 

* On Ubuntu 
Installing packages is straightforward. 
Very simple config file from ubuntu forums 

BUT, the node starts off in a draining state, because of low number of CPUs or something. 

prateeks@ubslurm1:~$ scontrol show node ubslurm1
NodeName=ubslurm1 Arch=x86_64 CoresPerSocket=1
   CPUAlloc=0 CPUErr=0 CPUTot=1 CPULoad=0.00
   AvailableFeatures=(null)
   ActiveFeatures=(null)
   Gres=(null)
   NodeAddr=ubslurm1 NodeHostName=ubslurm1 Version=17.11
   OS=Linux 4.15.0-1026-gcp #27-Ubuntu SMP Thu Dec 6 18:27:01 UTC 2018
   RealMemory=1 AllocMem=0 FreeMem=5727 Sockets=1 Boards=1
   State=IDLE+DRAIN ThreadsPerCore=2 TmpDisk=0 Weight=1 Owner=N/A MCS_label=N/A
   Partitions=long
   BootTime=2019-02-19T14:32:18 SlurmdStartTime=2019-02-19T15:20:27
   CfgTRES=cpu=1,mem=1M,billing=1
   AllocTRES=
   CapWatts=n/a
   CurrentWatts=0 LowestJoules=0 ConsumedJoules=0
   ExtSensorsJoules=n/s ExtSensorsWatts=0 ExtSensorsTemp=n/s
   Reason=Low socket*core*thread count, Low CPUs [slurm@2019-02-19T15:11:41]


Image: ubs1 

munge fails  ugh who knows why 

Step1 : set auth/none instead of auth/munge in slurm.conf 

Now, srun hostname fails with invalid job credential . Needs munge running in some strange order ? Magically works now. 

** MPI


* New master 
munge key must have the slurm master host name. Replace @ubslurm1

remove /var/lib/slurm-llnl/slurmctld/clustername first time starting a master 

TODO: IP address in the callback sripts handle_fin/fail.sh 
