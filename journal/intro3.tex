%\vspace*{\largesubsecspace}
\section{Introduction}
\label{sec:intro}

%\noindent \textbf{Scientific Computing On Transient VMs.}
Increasingly, cloud computing platforms have begun to supplement and complement conventional high performance computing (HPC) infrastructure to meet the large computing and storage requirements of scientific computing applications. Public cloud platforms such as Amazon's EC2, Google Cloud Platform, and Microsoft Azure, offer multiple benefits such as \emph{on-demand} resource allocation, convenient pay-as-you-go pricing models, ease of provisioning and deployment, and near-instantaneous elastic scaling. 
However, the flexibility offered by cloud platforms comes at a literal cost: the price of deploying scientific computing applications can be significant, and is a major hurdle towards adoption. 

Conventionally, cloud VMs have been offered with ``on-demand'' availability, such that the lifetime of the VM is solely determined by the owner of the VM (i.e., the cloud customer). 
Increasingly however, cloud providers have begun offering low-cost VMs with \emph{transient}, rather than continuous on-demand availability. 
Transient VMs can be unilaterally revoked and preempted by the cloud provider, and applications running inside them face fail-stop failures. 
Due to their volatile nature, transient VMs are offered at steeply discounted rates. Amazon EC2 spot instances, Google Cloud Preemptible VMs, and Azure Batch VMs, are all examples of transient VMs, and are offered at discounts ranging from 50 to 90\%.  


Deploying applications on such transient VMs requires new mechanisms and policies for mitigating preemptions, which also minimize the total running time and cost.
%
In this paper, we consider a \emph{new} kind of transient availability, which we call \emph{temporally constrained preemptions}.
In this availability model, a transient VM has a \emph{fixed} upper bound on its lifetime.
Google's Preemptible VMs obey such a model: they have a maximum lifetime of 24 hours, and can be preempted at any point within the $[0, 24]$ hour interval.
While these Preemptible VMs are 80\% cheaper than their regular non-preemptible counterparts, their effective use requires overcoming several hurdles, which we seek to achieve.

\noindent \textbf{Temporally Constrained Preemption Challenges.}
The \emph{first} major challenge lies in understanding and developing a preemption model: when and how frequently do preemptions occur, and what does their distribution look like?
Developing such a model is a vital precursor to time and cost minimizing policies, which have been developed for other preemption models (such as for Amazon EC2 spot instances). 
To this end, we conduct a first-of-its-kind empirical study of observing actual VM preemptions, and analyze the resulting data from more than 850 preemption events.
Our dataset is open-source and available at ~\cite{scispot-dataset}. 
Our findings indicate that due to the temporal constraint, the distribution of preemptions is unlike any other classical failure distribution (such as exponential or Weibull).
Instead, the failure distribution is ``bathtub'' shaped, and has multiple distinct phases: preemptions are more likely to occur at the start and end of the 24 hour interval. 
This has serious implications for prior transient-computing software, since they all assume exponentially distributed failures, and are unable to efficiently deal with the time-varying failure rates.

The \emph{second} major challenge is developing an analytical model for temporally constrained preemptions.
Such a model is essential for characterizing failures and obtaining key reliability metrics such as Mean-Time-To-Failure (MTTF), hazard and survival rate, etc.
We therefore develop a simple, generalizable, and differentiable analytical model for constrained preemptions, which provides new insights into preemption/failure dynamics.

Given the prevalence of bathtub-shaped preemptions, the \emph{third} major challenge is to understand \emph{why} they occur, and what the fundamental causes could be. 
For this, we find a surprising connection with \emph{Statistical Mechanics}!
We show that constrained preemptions can be ``mapped'' to the Tonks gas model~\cite{tonks, krauth2006statistical}, which describes a system of non-overlapping particles constrained to move within a line segment. The bathtub shape arises naturally if we assume non-overlapping preemptions. 

The \emph{fourth} challenge is reducing the cost and running time of applications deployed on Preemptible VMs.
We leverage our analytical preemption model and develop new optimized policies for scheduling and cost-optimization, that mitigate preemptions using insights gleaned from our models.


\emph{Finally}, we address the practical challenges of deploying scientific computing batch applications on Preemptible VMs. 
We identify a new simple abstraction, which we call ``bags of jobs'', which is common in many scientific simulation workloads. 
This abstraction, combined with our optimized policies, allows us to significantly reduce the cost of cloud computing resources, by up to $5\times$.
Our resultant system, SciSpot, implements all our policies and abstractions, and provides the first seamless and frictionless service for using Preemptible VMs. 


\noindent \textbf{Novelty and Relevance.}
%
\emph{Spot markets} (used by Amazon EC2's spot instances and others) are a popular transient computing model, where preemptions are governed by dynamic prices (which are in turn set using a continuous second-price auction~\cite{spot-pricing2}).
However, the temporally constrained preemption model is distinct from spot markets, and presents fundamental challenges in preemption modeling and effective use of transient VMs. 
Transiency-mitigation techniques such as VM migration~\cite{spotcheck}, checkpointing~\cite{flint, marathe2014exploiting}, diversification~\cite{exosphere}, \emph{all} use price-signals to model the availability and preemption rates of spot instances. 
With flat pricing, these approaches are not applicable. 
Furthermore, no other information about preemption characteristics is made publicly available by the cloud operator, not even coarse-grained metrics, which necessitates our empirical approach. 


To expand the usability and appeal of transient VMs, many systems and techniques have been proposed that seek to ameliorate the effects of preemptions and reduce the computing costs of applications. 
Fault-tolerance mechanisms~\cite{spotcheck, marathe2014exploiting}, resource management policies~\cite{exosphere, conductor}, and cost optimization techniques~\cite{dubois2016optispot, shastri2017hotspot} have been proposed for a wide range of applications---ranging from interactive web services, distributed data processing, parallel computing, etc.
However, these prior works all assume classical exponential failures, which we show to not be ideal for bathtub preemptions. 


We make significant analytical, theoretical, and practical enhancements to our preliminary work on understanding constrained preemptions~\cite{scispot-hpdc20}. 
We provide new empirical insights about preemption dynamics by analyzing the data along a larger number of dimensions.
We enhance our analytical model to compute closed-form expressions for MTTF and hazard rate, which are the key reliability metrics used in our policies. 
We introduce the use of statistical mechanics principles to elucidate the fundamental origins of the bathtub shape of temporally constrained preemptions.
We introduce a new ``tipping points'' based VM scheduling policy which has a key practical benefit of not requiring exact job running times. 
Finally, we conduct extensive empirical evaluation and show SciSpot's performance and cost, and compare it to the state of the art transient-computing systems. 



\begin{comment}
cloud platforms presents multiple challenges due to the  \emph{fundamental} differences with conventional HPC clusters---which most applications still assume as their default execution environment.
While the on-demand resource provisioning and pay-as-you-go pricing makes it easy to spin-up computing clusters in the cloud, the deployment of applications must be cognizant of the heterogeneity in VM sizes, pricing, and VM availability. 
Crucially, optimizing for \emph{cost} in addition to makespan,  becomes an important objective in cloud deployments. 
Furthermore, although using transient resources can drastically reduce computing costs, their preemptible nature results in frequent job failures. 
Preemptions can be mitigated with additional fault-tolerance mechanisms and policies~\cite{flint, marathe2014exploiting}, although they impose additional performance and deployment overheads. 

%\noindent \textbf{Transient Computing Challenges.}
To expand the usability and appeal of transient VMs, many systems and techniques have been proposed that seek to ameliorate the effects of preemptions and reduce the computing costs of applications. 
Fault-tolerance mechanisms~\cite{spotcheck, marathe2014exploiting}; resource management policies~\cite{exosphere, conductor}; and cost optimization techniques~\cite{dubois2016optispot, shastri2017hotspot} have been proposed for a wide range of applications---ranging from interactive web services, distributed data processing, parallel computing, etc.
These techniques have been shown to minimize the performance-degradation and downtimes due to preemptions, and reduce computing costs by up to 90\%. 
However, the success of these techniques depends on probabilistic estimates of when and how frequently preemptions occur. 
For instance, many fault-tolerance and resource optimization policies are parametrized by the mean time to failure (MTTF) of the transient VMs.

%A commonly used technique in transient computing is to periodically checkpoint application state, and the ``optimal'' checkpointing frequency that minimizes the total expected running time of a job depends on the MTTF of the VMs~\cite{daly2006higher}. 


%\noindent \textbf{Challenges with Google Preemptible VMs.}
\emph{Spot markets} (used by Amazon EC2's spot instances and others) are a popular model, where preemptions are governed by dynamic prices (which are in turn set using a continuous second-price auction~\cite{spot-pricing2}).
%
In this paper, we focus on a different transient availability model---temporally constrained preemptions, which are found in Google Preemptible VMs. 
In this model, transient VMs have a fixed maximum lifetime, that acts as a temporal constraint on the preemption events. 
Google's Preemptible VMs are temporally constrained---they have a maximum lifetime of 24 hours, and are always preempted within the $[0,24]$ hour interval.
%
The \textbf{temporally constrained preemption} model is distinct from spot markets, and presents fundamental challenges in preemption modeling and effective use of transient VMs. 
Transiency-mitigation techniques such as VM migration~\cite{spotcheck}, checkpointing~\cite{flint, marathe2014exploiting}, diversification~\cite{exosphere}, \emph{all} use price-signals to model the availability and preemption rates of spot instances. 
With flat pricing, these approaches are not applicable. 
Furthermore, no other information about preemption characteristics is publicly available, not even coarse-grained metrics. 
To address this, we develop an \emph{empirical} approach for understanding and modeling preemptions. 
We conduct a large empirical study of over 800 preemptions of Google Preemptible VMs, and develop an analytical probability model for temporally constrained preemptions. 


Due to the temporal constraint on preemptions, classical models that form the basis of preemption modeling and policies, such as memoryless exponential failure rates, are not applicable. 
%
We find that preemption rates are \emph{not} uniform, but bathtub shaped with multiple distinct temporal phases, and are incapable of being modeled by existing bathtub distributions such as Weibull.
%
We capture these characteristics by \textbf{developing a new probability model}. 
Our model uses reliability theory principles to capture the 24-hour lifetime of VMs, and generalizes to VMs of different resource capacities, geographical regions, and across different temporal domains.


In addition to empirical modeling, we also develop a new theoretical framework for understanding temporally constrained preemptions, by using principles and results from the field of \textbf{statistical mechanics.} 
Specifically, we show that the bathtub failures of constrained preemptions can be explained using the physics of the Tonks gas model~\cite{tonks, krauth2006statistical}. 


% Using our probability model, we find that bathtub failures can reduce the recomputation overhead of preeemptions by more than  $10\times$ compared to uniform failures---which has important implications for cloud users and providers. 


We show the applicability and effectiveness of our model by developing \textbf{optimized policies} for job scheduling and cost optimization. 
These policies are fundamentally dependent on empirical and analytical insights from our model. % such as different time-dependent failure rates of different types of VMs.
%
Our job-scheduling policy uses the bathtub behavior to decide whether to run a new job on a running VM or to request a new VM, and reduces job-failure probability by $2\times$ compared to  conventional memoryless policies. 
%
%The bathtub distribution also requires a new approach to periodic checkpointing---since existing Young-Daly~\cite{daly2006higher} checkpointing is restricted to memoryless preemptions. 
%
%Our checkpointing policy combines our preemption model and dynamic programming to reduce the checkpointing overhead by more than $5\times$. 
%
These optimized policies act as building blocks for transient computing systems and for reducing the performance degradation and costs of preemptible VMs. 
%and we show that the existing exponential ones are not suitable? 


We implement and evaluate these policies as part of a batch computing service, \textbf{SciSpot}, which we also use for empirically evaluating the effectiveness of our model and policies under real-world conditions. 
%
\sysname abstracts typical scientific computing workloads and workflows into a new unit of execution, which we call as a ``bag of jobs''. 
These bags of jobs, ubiquitous in scientific computing, represent multiple instantiations of the same application launched with possibly different physical and computational parameters.
The bag of jobs approach is also central to the rapidly developing area of using machine learning (ML) to enhance scientific computing applications, in which the same application is run with different parameters to curate ``training data sets'' for designing the ML-based enhancements (e.g., surrogates, force fields) ~\cite{kadupitiya2019machine,kadupitiya2020machine2,moradzadeh2019molecular,fox2019learning,wang2019machine,casalino2021ai,kadupitiya2020machine}.
%
The bag of jobs abstraction permits efficient implementation of our optimized policies, and allows \sysname to lower the costs and barriers of transient VMs for scientific computing applications.

\end{comment}

\noindent \textbf{Contributions.}
%
Towards our goal of harnessing temporally constrained transient VMs for modern scientific computing workloads, 
% developing a better understanding of constrained preemptions,
we make the following contributions:
%\vspace*{\largesubsecspace}
\begin{enumerate} [leftmargin=12pt]
\item Using a large-scale empirical study of Google's Preemptible VMs, we show a statistical analysis of preemptions based on the VM type, temporal effects, geographical regions, etc. Our analysis indicates that the 24-hour constraint is a defining characteristic, and that the preemption rates are \emph{not} uniform, but have distinct phases. 

\item We develop a probability model of constrained preemptions based on empirical and statistical insights that point to distinct failure processes underpinning the preemption rates. Our model captures the key effects resulting from the 24 hour lifetime constraint associated with these VMs.

\item We analyze our probability model through the lens of reliability theory, and demonstrate the use of the principles of statistical mechanics to examine the fundamental behavior of constrained preemptions. 
  
  %and statistical mechanics. 

%  and enables accurate prediction of expected running times and costs of different scientific computing applications.
% WTF is even partial redundancy? 

\item Based on our preemption model, we develop optimized policies for job scheduling which minimize the total time and cost of running applications. These policies reduce job running times by up to $2\times$ compared to existing preemption models used for transient VMs. 
  
%\item In order to select the optimal VM for an application, from the plethora of choices offered by cloud providers, we develop a transient VM selection policy that minimizes the cost of running applications. Our search based policy selects a transient VM based on it's cost, performance, and preemption rate. 

\item We implement and evaluate our policies as part of a batch computing service for Google Preemptible VMs. SciSpot introduces the bags of jobs abstraction for scientific simulation applications, and can reduce computing costs by $5\times$ compared to conventional cloud deployments, and lower the performance overhead of preemptible VMs to less than $3\%$. 
%  and reduce job failure probability by up to  $2\times$. 

 
%\item Finally, ease of use and extensibility are one of the ``first principles'' in the design of \sysname, and we present the design and implementation of the system components and present case studies of how  scientific applications such as molecular dynamics simulations can be easily deployed on transient cloud VMs. 
\end{enumerate}

%\footnotetext{Preemption dataset available at https://github.com/kadupitiya/goog-preemption-data/}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:

