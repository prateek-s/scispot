%\vspace*{\largesubsecspace}
\section{Introduction}
\label{sec:intro}

%\noindent \textbf{Scientific Computing On Transient VMs.}
Increasingly, cloud computing platforms have begun to supplement and complement conventional HPC infrastructure to meet the large computing and storage requirements of scientific applications. Public cloud platforms such as Amazon's EC2, Google Cloud Platform, and Microsoft Azure, offer multiple benefits such as \emph{on-demand} resource allocation, convenient pay-as-you-go pricing models, ease of provisioning and deployment, and near-instantaneous elastic scaling. 
However, the flexibility offered by cloud platforms comes at a literal cost: the price of deploying scientific computing applications can be significant, and continues to be a major hurdle towards adoption. 

Conventionally, cloud VMs have been offered with ``on-demand'' availability, such that the lifetime of the VM is solely determined by the owner of the VM (i.e., the cloud customer). 
Increasingly however, cloud providers have begun offering low-cost VMs with \emph{transient}, rather than continuous on-demand availability. 
Transient VMs can be unilaterally revoked and preempted by the cloud provider, and applications running inside them face fail-stop failures. 
Due to their volatile nature, transient VMs are offered at steeply discounted rates. Amazon EC2 spot instances, Google Cloud Preemptible VMs, and Azure Batch VMs, are all examples of transient VMs, and are offered at discounts ranging from 50 to 90\%.  

However, deploying applications on cloud platforms presents multiple challenges due to the  \emph{fundamental} differences with conventional HPC clusters---which most applications still assume as their default execution environment.
While the on-demand resource provisioning and pay-as-you-go pricing makes it easy to spin-up computing clusters in the cloud, the deployment of applications must be cognizant of the heterogeneity in VM sizes, pricing, and VM availability. 
Crucially, optimizing for \emph{cost} in addition to makespan,  becomes an important objective in cloud deployments. 
Furthermore, although using transient resources can drastically reduce computing costs, their preemptible nature results in frequent job failures. 
Preemptions can be mitigated with additional fault-tolerance mechanisms and policies~\cite{flint, marathe2014exploiting}, although they impose additional performance and deployment overheads. 

%\noindent \textbf{Transient Computing Challenges.}
To expand the usability and appeal of transient VMs, many systems and techniques have been proposed that seek to ameliorate the effects of preemptions and reduce the computing costs of applications. 
Fault-tolerance mechanisms~\cite{spotcheck, marathe2014exploiting}; resource management policies~\cite{exosphere, conductor}; and cost optimization techniques~\cite{dubois2016optispot, shastri2017hotspot} have been proposed for a wide range of applications---ranging from interactive web services, distributed data processing, parallel computing, etc.
These techniques have been shown to minimize the performance-degradation and downtimes due to preemptions, and reduce computing costs by up to 90\%. 
However, the success of these techniques depends on probabilistic estimates of when and how frequently preemptions occur. 
For instance, many fault-tolerance and resource optimization policies are parametrized by the mean time to failure (MTTF) of the transient VMs.

%A commonly used technique in transient computing is to periodically checkpoint application state, and the ``optimal'' checkpointing frequency that minimizes the total expected running time of a job depends on the MTTF of the VMs~\cite{daly2006higher}. 


%\noindent \textbf{Challenges with Google Preemptible VMs.}
\emph{Spot markets} (used by Amazon EC2's spot instances and others) are a popular model, where preemptions are governed by dynamic prices (which are in turn set using a continuous second-price auction~\cite{spot-pricing2}). 
In this paper, we focus on a different transient availability model---temporally constrained preemptions, which are found in Google Preemptible VMs. 
In this model, transient VMs have a fixed maximum lifetime, that acts as a temporal constraint on the preemption events. 
Google's Preemptible VMs are temporally constrained---they have a maximum lifetime of 24 hours, and are always preempted within the $[0,24]$ hour interval.
%
The \textbf{temporally constrained preemption} model is distinct from spot markets, and presents fundamental challenges in preemption modeling and effective use of transient VMs. 
Transiency-mitigation techniques such as VM migration~\cite{spotcheck}, checkpointing~\cite{flint, marathe2014exploiting}, diversification~\cite{exosphere}, \emph{all} use price-signals to model the availability and preemption rates of spot instances. 
With flat pricing, these approaches are not applicable. 
Furthermore, no other information about preemption characteristics is publicly available, not even coarse-grained metrics. 
To address this, we develop an \emph{empirical} approach for understanding and modeling preemptions. 
We conduct a large empirical study of over 800 preemptions of Google Preemptible VMs, and develop an analytical probability model for temporally constrained preemptions. 


Due to the temporal constraint on preemptions, classical models that form the basis of preemption modeling and policies, such as memoryless exponential failure rates, are not applicable. 
%
We find that preemption rates are \emph{not} uniform, but bathtub shaped with multiple distinct temporal phases, and are incapable of being modeled by existing bathtub distributions such as Weibull.
%
We capture these characteristics by \textbf{developing a new probability model}. 
Our model uses reliability theory principles to capture the 24-hour lifetime of VMs, and generalizes to VMs of different resource capacities, geographical regions, and across different temporal domains.


In addition to empirical modeling, we also develop a new theoretical framework for understanding temporally constrained preemptions, by using principles and results from the field of \textbf{statistical mechanics.} 
Specifically, we show that the bathtub failures of constrained preemptions can be explained using the physics of the Tonks gas model~\cite{tonks, krauth2006statistical}. 


% Using our probability model, we find that bathtub failures can reduce the recomputation overhead of preeemptions by more than  $10\times$ compared to uniform failures---which has important implications for cloud users and providers. 


We show the applicability and effectiveness of our model by developing \textbf{optimized policies} for job scheduling and cost optimization. 
These policies are fundamentally dependent on empirical and analytical insights from our model. % such as different time-dependent failure rates of different types of VMs.
%
Our job-scheduling policy uses the bathtub behavior to decide whether to run a new job on a running VM or to request a new VM, and reduces job-failure probability by $2\times$ compared to  conventional memoryless policies. 
%
%The bathtub distribution also requires a new approach to periodic checkpointing---since existing Young-Daly~\cite{daly2006higher} checkpointing is restricted to memoryless preemptions. 
%
%Our checkpointing policy combines our preemption model and dynamic programming to reduce the checkpointing overhead by more than $5\times$. 
%
These optimized policies act as building blocks for transient computing systems and for reducing the performance degradation and costs of preemptible VMs. 
%and we show that the existing exponential ones are not suitable? 


We implement and evaluate these policies as part of a batch computing service, \textbf{SciSpot}, which we also use for empirically evaluating the effectiveness of our model and policies under real-world conditions. 
%
\sysname abstracts typical scientific computing workloads and workflows into a new unit of execution, which we call as a ``bag of jobs''. 
These bags of jobs, ubiquitous in scientific computing, represent multiple instantiations of the same application launched with possibly different physical and computational parameters.
The bag of jobs is especially useful for machine-learning integrated scientific computing applications, in which the same application is run with different parameters to curate ``training data sets'' for surrogates~\cite{kadupitiya2020machine2} of physical systems.
%
The bag of jobs abstraction permits efficient implementation of our optimized policies, and allows \sysname to lower the costs and barriers of transient VMs for scientific computing applications.


Towards our goal of harnessing temporally constrained transient VMs for modern scientific computing workloads, 
% developing a better understanding of constrained preemptions,
we make the following contributions:
%\vspace*{\largesubsecspace}
\begin{enumerate} [leftmargin=12pt]
\item Using a large-scale empirical study of Google's Preemptible VMs \footnotemark, we show a statistical analysis of preemptions based on the VM type, temporal effects, geographical regions, etc. Our analysis indicates that the 24-hour constraint is a defining characteristic, and that the preemption rates are \emph{not} uniform, but have distinct phases. 

\item We develop a probability model of constrained preemptions based on empirical and statistical insights that point to distinct failure processes underpinning the preemption rates. Our model captures the key effects resulting from the 24 hour lifetime constraint associated with these VMs.

\item We analyze our probability model through the lens of reliability theory, and develop a theory of contrained preemptions using the principles of statistical mechanics. 

  
  %and statistical mechanics. 

%  and enables accurate prediction of expected running times and costs of different scientific computing applications.
% WTF is even partial redundancy? 

\item Based on our preemption model, we develop optimized policies for job scheduling and checkpointing that minimize the total time and cost of running applications. These policies reduce job running times by up to $2\times$ compared to existing preemption models used for transient VMs. 
  
%\item In order to select the optimal VM for an application, from the plethora of choices offered by cloud providers, we develop a transient VM selection policy that minimizes the cost of running applications. Our search based policy selects a transient VM based on it's cost, performance, and preemption rate. 

\item We implement and evaluate our policies as part of a batch computing service for Google Preemptible VMs. SciSpot introduces the bag of jobs abstraction for scientific simulation applications, and can reduce computing costs by $5\times$ compared to conventional cloud deployments, and reduce the performance overhead of preemptible VMs to less than $3\%$. 
%  and reduce job failure probability by up to  $2\times$. 

 
%\item Finally, ease of use and extensibility are one of the ``first principles'' in the design of \sysname, and we present the design and implementation of the system components and present case studies of how  scientific applications such as molecular dynamics simulations can be easily deployed on transient cloud VMs. 
\end{enumerate}

\footnotetext{Preemption dataset available at https://github.com/kadupitiya/goog-preemption-data/}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:

