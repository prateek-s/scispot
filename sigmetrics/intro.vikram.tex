\section{Introduction}

% Look, scientific applications are important, OK? And HPC. 

\eat{Scientific computing applications are a crucial component in the advancement of science and engineering, and play an important role in the analysis and processing of data, and understanding and modeling natural processes. 
These applications are typically implemented as large-scale parallel programs that use parallel-computing communication and coordination frameworks such as MPI.
To take advantage of their parallel nature, conventionally, these applications have mostly been deployed on large, dedicated high performance computing infrastructure such as super computers. }
%Logical flow here is not obvious and missing something. 

Scientific computing applications play a critical role in understanding natural and synthetic phenomena associated with a wide range of material, biological, and engineering systems.
%
The computational models and simulations for analyzing these systems can consume a large amount of computing resources, and require access to large dedicated high performance computing (HPC) infrastructure. 



\eat{Application of computer simulation plays a critical role in understanding natural and synthetic phenomena associated with a wide range of material, biological, and engineering systems. This scientific computing approach involves the analysis and processing of data generated by the mathematical model representations of these systems implemented on computers. Typically, these scientific computing applications (simulations) are designed as parallel programs that leverage the communication and coordination frameworks associated with parallel computing techniques such as MPI in order to yield useful information at a faster pace (shorter user time). To exploit the parallel processing capabilities, such applications are routinely deployed on large, dedicated high performance computing infrastructure.}



%such as supercomputers~\cite{bigred2}.


% VJ: perhaps it could be useful to point out that the cloud approach will not require any further code changes like one encounters in moving from sequential to MPI appraoch; but it would need a framework innovation

% But now we have the cloud!!1

Increasingly, cloud computing platforms have begun to supplement and complement conventional HPC infrastructure  to meet the large computing and storage requirements of scientific applications. Public cloud platforms such as Amazon's EC2, Google Cloud Platform, and Microsoft Azure, offer multiple benefits such as \emph{on-demand} resource allocation, convenient pay-as-you-go pricing models, ease of provisioning and deployment, and near-instantaneous elastic scaling.
Most cloud platforms offer \emph{Infrastructure as a Service}, and provide computing resources in the form of \emph{Virtual Machines (VMs)}, on which a wide range of  applications such as web-services, distributed data processing, distributed machine learning, etc., are deployed.


% a wide range of applications.

% Vikram's
\eat{
The extensive use of cloud platforms to host and run a wide range of applications such as web-services and distributed data processing have inspired early investigations in the direction of using these resources for scientific computing applications to meet the large computing and storage requirements of the latter. Early work in this area has shown the potential of using these cloud platforms to both supplement and complement conventional HPC infrastructure. Public cloud platforms such as Amazon's EC2, Google Cloud Platform, and Microsoft Azure, offer multiple benefits: \emph{on-demand} resource allocation, convenient pay-as-you-go pricing models, ease of provisioning and deployment, and near-instantaneous elastic scaling, to name a few. Most cloud platforms offer \emph{Infrastructure as a Service}, and provide computing resources in the form of Virtual Machines (VMs) that are application-agnostic and can serve as deployment sites for a wide range of applications such as web-services, distributed machine learning, and scientific simulations. 
}

% Clouds are complex, and also have transience

To  meet the diverse resource demands of different applications, public clouds offer resources (i.e., VMs) with multiple different resource configurations (such as number of CPU cores,  memory capacity, etc.), and pricing and availability contracts. 
Conventionally, cloud VMs have been offered with ``on-demand'' availability, such that the lifetime of the VM is solely determined by the owner of the VM (i.e., the cloud customer). 
Increasingly however, cloud providers have begun offering VMs with \emph{transient}, rather than continuous on-demand availability. 
Transient VMs can be unilaterally revoked and preempted by the cloud provider, and applications running inside them face fail-stop failures. 
Due to their volatile nature, transient VMs such as Amazon Spot instances, Google Preemptible VMs, and Azure Batch VMs, are offered at steeply discounted rates ranging from 50 to 90\%.
%Amazon EC2 spot instances, Google Cloud Preemptible VMs, and Azure Batch VMs, are all examples of transient VMs, and are offered at discounts ranging from 50 to 90\%.  

% Very different from HPC and many challenges

\eat{
However, deploying applications on cloud platforms presents multiple challenges due to the  \emph{fundamental} differences with conventional HPC clusters---which most applications still assume as their default execution environment.
%
While the on-demand resource provisioning and pay-as-you-go pricing makes it easy to spin-up computing clusters in the cloud,  the deployment of applications on cloud platforms must be cognizant of the heterogeneity in VM sizes, pricing, and availability for effective resource utilization.
%
Crucially, optimizing for \emph{cost} in addition to  makespan  becomes an important objective in cloud deployments. 
% 
Furthermore, although using transient resources can drastically reduce computing costs, their preemptible nature results in frequent job failures.
%
Preemptions can be mitigated with additional fault-tolerance mechanisms and policies~\cite{flint, marathe2014exploiting}, although they impose additional performance and deployment overheads
}
%\vj{\it such as? needs one more sentence to clearly explain why these existing approaches are not addressing the need that we address in this paper below...}. 

%However, deploying applications on cloud platforms presents multiple challenges due to the  \emph{fundamental} differences with conventional HPC clusters---which most applications still assume as their default execution environment.
%
While the on-demand resource provisioning and pay-as-you-go pricing makes it easy to spin-up computing clusters in the cloud, the deployment of applications on cloud platforms must be cognizant of the heterogeneity in VM sizes, pricing, and availability for effective resource utilization. Crucially, optimizing for \emph{cost} in addition to makespan, becomes an important objective in cloud deployments. Furthermore, although using transient resources can drastically reduce computing costs, their preemptible nature results in frequent job failures. While preemptions can be mitigated with additional fault-tolerance mechanisms and policies~\cite{flint, marathe2014exploiting}, these policies must be typically tailored to the application~\cite{flint}, and impose additional performance and deployment overheads. 
%\vj{\it add one more sentence}. 
These considerations of cost, server configuration heterogeneity, and frequent job failures intrinsic to the system present multiple challenges in deploying applications on cloud platforms which are \emph{fundamentally} different from those that appear in using HPC clusters as the execution environment for the scientific computing applications.
% Finally getting to the point 

In this paper, we develop principled approaches for deploying and orchestrating  scientific computing applications on the cloud, and present \sysname, a framework for low-cost scientific computing on  transient cloud servers.

%
% What it is, vs. what problems we are actually solving. What's new?
%
Our policies for tackling the resource heterogeneity and transient availability of cloud VMs build on a key insight: most scientific computing applications are deployed as a collection or ``bag'' of jobs. 
%
These bags of jobs represent multiple instantiations of the same computation with different parameters. 
%
For instance, each job may be running a (parallel) simulation with a set of simulation input parameters, and different jobs in the collection run the same simulation employing a different set of  parameters. 
Collectively, a bag of jobs can be used to ``sweep'' or search across a multi-dimensional parameter space to discover or narrow down the set of feasible and viable parameters associated with the modeled natural or synthetic processes. A similar approach is adopted in the use of machine learning (ML) to enhance scientific computational methods, a rapidly emerging area of research, when a collection of jobs with independent parameter sets are launched to train ML models to predict simulation results and/or accelerate the simulation technique.

Prior approaches and systems for mitigating transiency and cloud heterogeneity have largely targeted individual instantiations of jobs~\cite{spoton, exosphere, flint, marathe2014exploiting}. 
%
For a bag of jobs, it is not necessary, or sufficient, to execute an individual job in timely manner---instead, we could selectively restart failed jobs in order to complete the necessary, desired subset of jobs in a bag. 
%
Furthermore, treating the bag of jobs as a fundamental unit of computation allows us to select the ``best'' server configuration for a given application, by exploring different servers for initial jobs and running the remainder of the jobs on the optimal server configuration.
%\vj{\it is this optimal server found on the fly after the initial set of explorations, or this can be found separately?}. 


%What we actually do is missing! Availability modeling in GCP, optimizing across all jobs, exploiting the partial redundancy among jobs to avoid checkpointing overheads, parameter sweeping techniques, early-stopping, checkpoint scheduling for non memoryless distributions. 
%
We show that optimizing across an entire bag of jobs and being cognizant of the relation between different jobs in a bag, can enable simple and powerful policies for optimizing cost, makespan, and ease of deployment. 
%
We implement these policies as part of the \sysname~framework, and make the following contributions:
%\vj{\it skipping the following contributions, will look at it after they are reordered and further refined}
%\vspace*{-6pt}
\begin{enumerate}[leftmargin=12pt]
\item In order to select the ``right'' VM from the plethora of choices offered by cloud providers, we develop a cluster configuration policy that minimizes the cost of running applications. Our search based policy selects a transient server type based on it's cost, parallel speedup, and probability of preemption. 

% Yeah the "not spot" point below may need clarification before contribs
  
\item Since transient server preemptions can disrupt the execution of jobs, we present the \emph{first} empirical model and analysis of transient server availability that is \emph{not} rooted in classical bidding models for EC2 spot instances that have been proposed thus far. Our empirical model allows us to predict expected running and costs of jobs of different types and duration.

% WTF is even partial redundancy? 
  
\item We develop preemption-mitigation policies to minimize the overall makespan of bags of jobs, by taking into consideration the partial redundancy between different jobs within a bag. Combined, our policies yield a cost saving of 70\%, and a makespan reduction of 20\% compared to a conventional HPC clusters. 

%\item Finally, ease of use and extensibility are one of the ``first principles'' in the design of \sysname, and we present the design and implementation of the system components and present case studies of how  scientific applications such as molecular dynamics simulations can be easily deployed on transient cloud VMs. 
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% We need to explain the bag of jobs metaphor very clearly here!

\begin{comment}
Our policies for  mitigating transiency build on one k


\sysname~runs unmodified parallel scientific applications, and is a cost and transience aware cluster manager. 




% What about the bag of jobs!?!?!

% And where is SciSpot!?!

% There are no references to related work, this is just a nice long story so far! 

%Heterogeneity of the types of VMs and pricing models. Cost becomes important. 


%Public clouds can supplement and complement the supercomputing infrastructure. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Running parallel scientific applications, such as molecular dynamics (MD) simulations, on low-cost cloud transient resources. 

The first "big" idea is that simulations are often bag of parallel tasks. 

While there has been some past work that looks at running MPI applications on spot instances, our scope is much broader and considers how complete simulation pipelines can be run at low cost. 

Spiel on transient instances. Increasingly popular resource allocation model that is being offered by all cloud providers. 
Very low cost compared to conventional cloud resources, often by up to 10x. 
However, can be frequently revoked. 
Thus failure is a common occurrence, and not a rare-event. 
This is especially challenging for MPI jobs because of its inability to tolerate failures. 

However, our insight is that while protecting a *single* job against revocations can require elaborate checkpointing based approaches, we dont necessarily have to do that if we consider that most simulations are composed of a series of jobs that search over a parameter space, and that what is important is the total running time and cost of this entire series of jobs. 

Thus, no single job is "special". 

Another aspect of novelty is that past work on transient resources used EC2 pricing information to get failure probabilities. However, this is no longer an accurate method. We perform the first empirical study of google preemptible VMs and their performance and availability for HPC workloads. 

Another fundamental question is what is a suitable metric in such cases. Conventionally, it is speedup. In the cloud, it is some combination of cost and running time. 


\sysname is a framework and a tool that combines the use of failure modeling, checkpointing, and application-aware early stopping, to provide low cost execution of jobgroups for scientific applications.


Our work is the first to make a principled study of transient instances \emph{other} than Amazon spot instances.
Furthermore, our techniques make the first stab at addressing the new problems in the new EC2 spot pricing scheme.

Our work is in the context of reliability and cloud execution of scientific applications, and is novel because of multiple reasons:
1. Reliability and failure analysis of parallel scientific applications usually studied in the context of hardware with MTTFs of centuries, which is several orders of magnitudes higher than MTTFs faced in transient cloud servers (few hours).

2. While there have been studies of scientific applications been deployed in the context of cloud platforms, to the best of our knowledge, there has been no effort that integrates server selection and running jobgroups in a convenient automatic manner that makes it feasible to actually deploy applications on the cloud for scientists who may not have the requisite cloud experience.
\end{comment}

\begin{comment}
Notes:
  
For scientific applications, public clouds offer many advantages such as no waiting/queuing time and instant access to a wide range of resources, and pay as you go pricing. 
However, judicious use of cloud resources is necessary to achieve high performance and to avoid cost overruns.

Increasingly, cloud providers are offering transient VMs that are sold at steeply discounted rates of 90\%.
However, they can be unilaterally revoked by the cloud provider, resulting in preemptions which are akin to fail-stop failures.
This is 


The inspiration for our work is the massively parallel molecular simulation pipelines that have been proposed recently that call for a large number of simulation parameters to be evaluated.
inspiration/case-study

Bag of jobs useful in many contexts. 


Easy to use and deploy tool that does not need special access to high performance computing systems. 

Cloud will be the key piece of software infrastructure for scientific applications.
However, the resource management and orchestration principles, techniques, and tools, are all different.
So in this paper, we design and develop mechanisms and policies for running these applications at \emph{low cost}.

Software framework.
Easy to run.
Auto-tuning and requires only specifying the program to run.
Auto-tunes resource allocation to minimize cost and running time.



Designing Materials to Revolutionize and Engineer ourFuture (DMREF) : Google cloud mention

CSSI: Robust service .
Framework implementation
 


\end{comment}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:

