
\section{Preemption-model Based Policies}

We propose to develop optimized preemption mitigating policies that make fundamental use of insights from our models and highlight their practical significance. 
Our goal is to develop a cost-optimizing execution platform for parallel batch applications, especially MPI-based scientific computing applications [6] whose large computing requirements and disruption-tolerance make them an ideal application for Preemptible VMs. 
In particular, the bathtub and time-dependent failures of Preemptible VMs require new policies for these fault-tolerance and resource management problems: 


\subsection{Bags of Jobs Framework}

\subsection{Job Scheduling}

\noindent \textbf{Scheduling} of application tasks is also affected by the bathtub nature and can be optimized using the analytical model. 
A job launched on an ``old''  VM faces a high risk preemption, and may want to run on a newly launched VM if the job length is larger than the expected remaining life of the time. 
However, since newly launched VMs also have high initial failure rates, the optimal choice depends on job-length and the preemption CDF.
This preemption-based scheduling is particularly effective in case of scientific simulations, where the job is repeatedly invoked to explore some parameter space, and job running times show little variance and can be accurately inferred. 
%For scientific computing applications (especially simulations), job lengths are often known: Many scientific computing applications, especially simuations, perform repeated execution on differet 

%and the preemption probability provided by our model.

Given a job of length T that must be started on a server that has been running for s time, the probability of failure on the existing server $P_{\text{existing}} = max(1, F(t+s) - F(t))$.
$P_{\text{new}} = F(t)$. We therefore decide new or existing server based on comparing the $P_{\text{existing}}, P_{\text{new}}$. 

\prat{What about parallel jobs? How many new servers to launch?}

\subsection{Periodic Checkpointing}

\noindent \textbf{Periodic checkpointing} is a common technique for alleviating preemptions, and optimizing the time interval between successive checkpoints is crucial for minimizing total running time. 
%Saving a checkpoint to disk slows down application execution; however, and rolling back state to the previous checkpoint results in re-running some computation. 
%Careful choice of the checkpointing interval can minimize the \emph{expected} time and cost. 
However, past work [1] has assumed preemptions have a memoryless exponential distribution, which results in sub-optimal checkpointing for Preemptible VMs with time-dependent failures. 
%Because Preemptible VMs have time-dependent failure rates and are not memoryless, existing techniques are sub-optimal. 
%\emph{The empirical and analytical findings from Aim 1 suggest a non-uniform failure rate.}
Our analytical preemption model can be used for designing an optimized checkpointing schedule. 
Intuitively, a dynamic programming approach can checkpoint at a rate proportional to preemption rate. %, and initial analysis indicates a reduction in the expected running time by up to 20\% compared to classical exponential distribution based techniques. 
We will implement this policy for MPI and Spark, and compare against prior checkpointing systems [2]. 
%the checkpointing rate should be proportional to the failure rate, and a dynamic programming approach can be used. 
%We believe that a dynamic programming approach can be used in combination with our analytical model, which also serves as an example of its immediately practical application.

\prat{Dynamic Programming Formulation}

Because of non-uniform failure rate, the periodic checkpointing with Young-Daly method is not appropriate.
The checkpointing frequency must be based on the failure probability.


Let the uninterrupted running time of the job be $T$. Or in other words, $T$ amount of work needs to be performed. 

Let the checkpoint cost be $\delta$. 
Then, the expected running time is sum of T, periodic checkpointing cost, and the expected recomputation.

We will recursively compute the makespan of the job.
Let $M(W, t, s)$ denote the makespan when $W$ length of job must be exectued, $t$ is amount of job successfully done, and $s$ was the starting time (wallclock time) of the job.


Goal is to find the checkpoint schedule.

The expected makespan when there are no failures,

Suppose $t$ units of the job have completed. 
If we take a checkpoint when $i$ units of the job have completed, then we have two cases.
Either there are no failures till the checkpoint period, or there is a failure before the checkpoint.

We denote the makespan with $M$.
Then, $M(a, b)$ denotes makespan of executing $a$ units of job, when $b$ units have already been successfully executed.
Therefore our goal is to find $M(T,0)$ .
We do this recursively, by finding the expectation of M and using linearity of expectation. 

There are two cases. 

$E[M_{\text{succ}}] = t+i+\delta + E[M(T-i, t+i+\delta)]$

$E[M_{\text{fail}}] = E[T_{\text{lost}}(i+\delta, t)] + E[M(T, t+i+\delta)]$

Here, $E[T_{\text{loss}}(i+\delta, t)]$ is the expected job running time that is ``lost'' due to the failure. In the case of memoryless distribution, this is approximated as $\frac{i+\delta}{2}$.
In our case, we use the failure CDF instead:

$E[T_{\text{loss}}(i+\delta, t)] = \int_{i+\delta}^{i+\delta+t}{xf(x)dx} $

We can now combine the two cases: 

$E[{C(i, t)}] = P_{\text{suc}}(i+\delta, t)E[M_{\text{succ}}] + P_{\text{fail}}(i+\delta, t)E[M_{\text{fail}}]$


Where $P_{\text{fail}}(i+\delta,t) = F(t+i+\delta)-F(i+\delta)$ is computed using the CDF, 

and $P_{\text{succ}} = 1 - P_{\text{fail}}$ 

We claim that $E[\mathcal{T}] = \min_{0 < i \leq T}{E[M(T,0)]}$


Now, we use a dynamic programming approach to find the $i$ that minimizes $E[C(i,t)]$



\subsection{VM Selection}

\noindent \textbf{VM-selection} is an important optimization in cloud environments, because VMs have different tradeoffs of cost, performance, and preemption characteristics.
Application performance is affected by the size of the VM (due to network communication and parallel scaling overheads), and the preemption rates. 
We propose to develop cost models for selecting the ``right'' type of  VMs that minimizes the expected job failure probability and cost by using the analytical preemption models. 
%using our model to \emph{compute the expected average lifetime of a VM} enabling us to minimize the impact of preemptions on performance and overall cost.
Initial analysis indicates that careful VM selection can reduce costs by  up to 30\%.  

\prat{Start old text}


%\vspace*{\subsecspace}
\subsubsection{Server Selection Policy}

%\sysname does an exhaustive search over all valid configurations to find the lowest-cost configuration $( i, n_i )$. 

\begin{comment}
We note that this search is different from conventional speedup plots in which the objective is to determine how well an application scales with increasing amount of resources and parallelism. 
In contrast, we \emph{fix} the total amount of resources allocated to the application's job ($=\mathcal{R}$), and only vary \emph{how} these resources are distributed, which affects communication overhead and hence the performance.
%Weak
We assume that the total resource requirement for a job, $\mathcal{R}$, is provided by the user based on prior speedup data, the user's cloud budget, and the deadline for job completion.  
\end{comment}

\vspace*{\subsecspace}
\subsubsection{Server Cost Model}
\label{subsec:cost-model}

Since server selection involves a tradeoff between cost, performance, and preemptions, we develop a model that allows us to optimize the resource allocation and pick the best VM type that minimizes the expected cost of running an application on \sysname. 


%Let $\mathcal{R}$ denote the total amount of computing resources requested for the job. For ease of exposition, let us assume that $\mathcal{R}$ is the total number of CPU cores.
%Furthermore, let $r_i$ denote the ``size'' of the server of type $i$.
%Then, the number of servers of type $i$ required, $n_i = \mathcal{R}/r_i$.
%In what follows, we denote the expectation value of a quantity as $E[\ldots]$.

%\vj{there is some repitition in defining the symbols here which are used before in selection policy; may be this can be moved above. was wondering if we loose clarity by using $T_k$ to denote the running time on configuration $k$ that encodes the pair defined by the combination of server type $i$ - number of servers of type $i$ -- $(i,n_i)$; that is, $k\equiv (i,n_i)$, used as a superindex?}

Let us assume that the cloud provider offers $N$ server types, with the price (per unit time) of a server type equal to $c_i$. 
The overall expected cost of running a job can then be expressed as follows:
\begin{equation}
  \label{eq:e-cost}
\vspace*{\eqnspace}
  E[C_{( i,n_i )}] = n_i\times c_i \times E[\mathcal{T}_{( i,n_i )}].
\end{equation}
Here, $E[\mathcal{T}_{( i,n_i )}]$ denotes the expected turnaround time of the job (accounting for preemptions) on $n_i$ servers of type $i$.
%
This turnaround time depends on whether the job needs to be recomputed because of preemptions, and is expressed as:
\begin{align}
  \label{eq:turnaround}
  \vspace*{\eqnspace}
  E[\mathcal{T}_{( i,n_i )}] &= T_{( i,n_i )} + E[\text{Recomputation Time}].
\end{align}
Here, $T_{( i,n_i )}$ is the base running time of a job without preemptions, which we obtain empirically as explained in the previous subsection.
Since jobs have to be rerun when they fail due to preemptions, the recomputation time is:
\begin{equation}
  \label{eq:recomput}
   E[\text{Recomputation Time}] = \frac{T_{( i,n_i )}}{2} \times P(\text{at least one preemption})
 \end{equation}
 Our expression of the recomputation time is based on the common assumption that jobs will fail at the half-way mark on average~\cite{daly2006higher, bougeret_checkpointing_2011}. 
%
 The probability that at least one VM out of $n_i$ will be preempted during the job execution is:
\begin{align}
  \label{eq:pfail1}
  P(\text{at least one preemption}) &= 1-P(\text{no preemptions}) \\
                                 &= 1-\left(1-P\left(i,T_{(i, n_i)}\right)\right)^{n_i}.
\end{align}

Here, $P(i, T_{(i, n_i)})$ denotes the probability of a preemption of a VM of type $i$ when a job of duration $T_{(i, n_i)}$ runs on it. 
%
It depends on the type of server, and its associated expected lifetime, and is defined as:
\begin{equation}
  \label{eq:pi}
  P\left(i, T_{\left(i, n_i \right)}\right) = \text{min}\left(\dfrac{T_{(i, n_i)}}{E[L_i]}, 1\right),
\end{equation}
where $E[L_i]$ is the expected lifetime of the VM of type $i$ extracted using the preemption model (Equation~\ref{eq:expected-lifetime}).
%As a first order approximation, the running time $t$ of the job can be chosen as $t=T_{( i,n_i )}$, where the latter is empirically obtained for a given application.
We also assume that the running time of \emph{individual} jobs in a bag ($T$), will be smaller than the expected lifetime of the VMs, otherwise we will see no forward progress since the jobs will always be preempted before completion.
This is a safe assumption, since more than 90\% HPC jobs are less than 2 hours long (Figure~\ref{fig:hpc-vs-scispot} inset), and the expected lifetime of transient VMs is more than 10 hours.
This restriction only applies to individual jobs---\sysname can run large bags of jobs even if their total running time exceeds the VM lifetime by replenishing preempted VMs.
%We again emphasize that $T$ is the running time of an \emph{individual} job, and that \sysname is designed for running large bags of small jobs, and that most HPC jobs are much smaller than the expected lifetimes, as we show in Section~\ref{sec:eval}. 



% Using Equations~\ref{eq:turnaround},\ref{eq:recomput}, and \ref{eq:pfail1}, the overall expected cost of running a job on transient cloud servers is obtained as:
% \begin{equation}
%   \label{eq:ecfinal}
%   E[C_{( i,n_i )}] = \frac{1}{2}n_i c_i T_{( i,n_i )}\left(3 - \left(1-\dfrac{T_{( i,n_i )}}{E[L_i]}\right)^{n_i}\right).
% \end{equation}

% Equation \ref{eq:ecfinal} shows that the expected cost $E[C]$ is higher for larger number of servers (high $n_i$), while it is reduced if the expected lifetime of the VM is larger (high $E[L_i]$).








%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
