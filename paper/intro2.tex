\vspace*{\largesubsecspace}
\section{Introduction}
\label{sec:intro}
% Look, scientific applications are important, OK? And HPC. 

%Scientific computing applications play a critical role in understanding natural and synthetic phenomena associated with a wide range of material, biological, and engineering systems. 
%The computational models and simulations for analyzing these systems can consume a large amount of computing resources, and require access to large dedicated high performance computing (HPC) infrastructure. 



%such as supercomputers~\cite{bigred2}.


% VJ: perhaps it could be useful to point out that the cloud approach will not require any further code changes like one encounters in moving from sequential to MPI appraoch; but it would need a framework innovation

% But now we have the cloud!!1

Scientific computing applications are a crucial component in the advancement of science and engineering, and play an important role in the analysis and processing of data, and understanding and modeling natural processes. 
% These applications are typically implemented as large-scale parallel programs that use parallel-computing communication and coordination frameworks such as MPI.
These applications are usually parallelized and require a large amount of computing resources (1000s of CPU cores), and are deployed on large, dedicated high performance computing (HPC) infrastructure such as super computers. 

Increasingly, cloud computing platforms have begun to supplement and complement conventional HPC  infrastructure  to meet the large computing and storage requirements of scientific computing applications~\cite{buyya-hpc-survey}. 
%
Applications can benefit from the multiple benefits of public clouds: on-demand resource allocation, convenient pay-as-you-go pricing models, and ease of provisioning and deployment on an ``infinite'' resource pool. 


%Most cloud platforms offer \emph{Infrastructure as a Service}, and provide computing resources in the form of virtual machines (VMs),  which run a wide range of applications such as web-services, distributed data processing, distributed machine learning, etc.

However, large deployment costs remains a concern for scientific computing. 
Cloud deployment costs can be reduced through the use of \emph{transient} computing resources (i.e., VMs) that can be unilaterally revoked and preempted by the cloud provider.
Due to their volatile nature, transient VMs such as Amazon EC2 Spot instances~\cite{spot-documentation}, Google Preemptible VMs~\cite{preemptible-documentation}, and Azure Batch VMs~\cite{azure-batch}, are offered at steeply discounted rates ranging from 50 to 90\%. 
Although using transient resources can drastically reduce computing costs, their preemptible nature results in frequent job failures, and thus reduces their viability and usability. 


Deploying scientific applications on cloud platforms presents multiple challenges due to the \emph{fundamental} differences with conventional HPC clusters.
HPC resource management frameworks such as Slurm~\cite{slurm} and Torque~\cite{torque} typically assume static, homogeneous clusters, and are not cognizant of transient availability and the plethora of choices in cloud VM sizes and pricing. %, for effective resource utilization. 
%---which most applications still assume as their default execution environment. 
%While the on-demand provisioning and pay-as-you-go pricing makes it easy to spin-up computing clusters in the cloud, the deployment of applications on cloud platforms must be cognizant of the plethora of choices in VM sizes, pricing, and availability. %, for effective resource utilization. 
Crucially, optimizing for \emph{cost} in addition to performance, is an important 
 objective in cloud deployments. 

%


% a wide range of applications.

% To meet the diverse resource demands of different applications, public clouds offer resources (i.e., VMs) with multiple different resource configurations (such as number of CPU cores,  memory capacity, etc.), and pricing and availability contracts.



\eat{
Conventionally, cloud VMs have been offered with ``on-demand'' availability, such that the lifetime of the VM is solely determined by the owner of the VM (i.e., the cloud customer). 
Increasingly however, cloud providers have begun offering VMs with \emph{transient}, rather than continuous on-demand availability. 
Transient VMs can be unilaterally revoked and preempted by the cloud provider, and applications running inside them face fail-stop failures. 
Due to their volatile nature, transient VMs such as Amazon EC2 Spot instances~\cite{spot-documentation}, Google Preemptible VMs~\cite{preemptible-documentation}, and Azure Batch VMs~\cite{azure-batch}, are offered at steeply discounted rates ranging from 50 to 90\%.
Although using transient resources can drastically reduce computing costs, their preemptible nature results in frequent job failures, and thus reduces their viability and usability. 
}

%Amazon EC2 spot instances, Google Cloud Preemptible VMs, and Azure Batch VMs, are all examples of transient VMs, and are offered at discounts ranging from 50 to 90\%.  

% Very different from HPC and many challenges

%

%While preemptions can be mitigated with additional fault-tolerance mechanisms and policies~\cite{flint, marathe2014exploiting}, these policies must be typically tailored to the application, and impose additional performance and deployment overheads, and thus reduce the viability and usability of transient resources. 

% \vj{\it add one more sentence}. 
%These considerations of cost, server configuration heterogeneity, and frequent job failures are intrinsic to transient servers, and  present multiple challenges in deploying applications on cloud platforms which are \emph{fundamentally} different from those that appear in using HPC clusters as the execution environment for the scientific computing applications.
% Finally getting to the point 


In this paper, we present \sysname, a framework that optimizes the deployment of scientific computing applications on transient cloud servers. 
% we develop principled approaches for deploying and orchestrating scientific computing applications on the cloud, and
%\prat{Optimized deployment on cloud transient}
%
\sysname introduces and incorporates policies for addressing the heterogeneity and preemptibility of transient cloud VMs, and is the \emph{first} framework for transient computing  that is not limited to Amazon EC2 spot instances, whose distinctive characteristics are inapplicable to other transient cloud VMs.

%where pricing signals have been used to develop preemption-mitigation policies.


% , and is suitable for low-cost, low-overhead scientific computing.

%running a wide range of scientific applications at low-cost and with minimal performance overhead.
%

\sysname can run scientific applications on transient Google Preemptible VMs, which raises a number of hitherto unexplored challenges. 
First, the preemption characteristics of these VMs, such as their time to preemption, is unspecified by the cloud provider, requiring extensive empirical study. 
Second, Google Preemptible VMs have a \emph{maximum} lifetime of 24 hours, which results in preemptions being constrained within a 24 hour window.
Understanding and modeling these preemptions requires new fundamental techniques, since existing reliability models (such as exponential and Weibull probability distributions for modeling lifespans) cannot meet the hard temporal constraints. 

To address these challenges and to develop cost and run-time minimizing resource allocation policies, we conduct large-scale experiments with over a thousand preemption events of Google Preemptible VMs, and develop a new reliability model for constrained preemption. 
This model introduces a new failure probability distribution, that allows us to mitigate preemptions, and predict expected running times and costs of different scientific computing applications. 

While several approaches for mitigating preemptions of transient cloud VMs have been proposed, they all rely on preemption information provided by Amazon EC2 spot prices.
Transiency-specific frameworks that use techniques such as VM migration~\cite{spotcheck}, checkpointing~\cite{flint, marathe2014exploiting}, diversification~\cite{exosphere}, \emph{all} use price-signals to model the availability and preemption rates of spot instances. 
However, these pricing-based models are not generalizable to other transient VMs having a flat price (such as Google's or Azure's offerings), or even across time, as Amazon has recently changed their pricing models~\cite{bid-change} and the applicability of price-based approaches remains uncertain~\cite{spotweb}. 
Unlike prior work on transient computing, \sysname's empirical preemption model allows effective use of a wider range of transient cloud resources (such as Google Preemptible VMs). 

%Due to its new empirical models, \sysname is the \emph{first} framework for transient computing  that is not limited to Amazon's EC2 spot instances, where pricing signals have been used to infer preemption events. 



\eat{
\sysname runs applications on Google Cloud Preemptible VMs and addresses the unique reliability and systems challenges arising from their maximum 24-hour lifetime enforced by the cloud platform.
While several techniques for mitigating revocations of transient VMs have been developed~\cite{}, they are all designed for Amazon's EC2 spot market, and rely on information about the past availability inferred through the dynamic spot price published by Amazon. 
These techniques \emph{cannot} be directly applied in the case of other cloud transient VMs, such as Microsoft's Batch VMs and Google's Preemptible VMs, in which availability of VMs cannot be inferred through dynamic pricing.
}

\eat{
To address this gap and to develop cost and run-time minimizing resource allocation policies,
we develop an empirical model derived from over a thousand preemption events of Google Preemptible VMs, that allows us to predict expected running times and costs of different scientific computing applications. 
To the best of our knowledge, this is the \emph{first} empirical analysis and model of preemptions that is not based on classical EC2 spot instance bidding.
}

%models for EC2 spot instances that have been proposed thus far.

%We develop an empirical model derived from over a thousand preemption events, which allows us to predict expected running times and costs of different scientific computing applications. %, which informs our bags of jobs execution policies.



%Where they are found. Needs to be crisper 
%Bags of jobs are ubiquitous in scientific computing: 
%for example, each job may be running a (parallel) simulation with a set of simulation input parameters, and different jobs in the collection (i.e., bag) run the same simulation with a different set of  parameters. 

\sysname abstracts typical scientific computing workloads and workflows into a new unit of execution, which we call as a ``bag of jobs''. 
These bags of jobs, ubiquitous in scientific computing, represent multiple instantiations of the same application launched with possibly different physical and computational parameters. 
Collectively, a bag of jobs can be used to ``sweep'' or search across a multi-dimensional parameter space to isolate the set of desired parameters associated with the scientific computation model. 
Bags of jobs also help in the use of machine learning (ML) to enhance scientific computational methods ~\cite{ml.atomic2017,melko2017,sam2017,fu2017,long2015machine, ferguson2017machine,ward2018matminer}, when a collection of jobs with different  parameters  are launched to train and test ML models.
%independent parameter sets 
%to predict simulation results and/or accelerate the simulation technique. 


% Not too happy with the last sentence and paragraph

%
Treating bag of jobs as a fundamental unit of computation enables \sysname to select the ``best'' server configuration for a given application that minimizes the costs, running time, and preemption likelihood.
%, by exploring different servers for initial jobs and running the remainder of the jobs on the optimal server configuration. 
%Unlike prior work that assumes that the optimal resource allocation for an application is known a priori, 
\sysname can find the optimal allocation on-the-fly by exploring different servers for initial jobs and running the remainder of the jobs on the optimal server configuration. 
%
%Thus, while recently developed transiency mitigation techniques such as
For a bag of jobs, it is not necessary, or sufficient, to execute an individual job in timely manner---instead, we can selectively restart failed jobs in order to complete the necessary, desired subset of jobs in a bag.
In contrast, prior work on transiency-mitigation has mostly focused on fault-tolerance and resource management for a \emph{single} job or application~\cite{spoton, exosphere, flint, marathe2014exploiting}. 

%Thus, while recently developed techniques such as transiency-aware checkpointing~\cite{marathe2014exploiting, flint} and diversification~\cite{exosphere, spotweb} can indeed mitigate the effect of preemptions on a single job, these techniques are not tailored for the bags of jobs execution model. 


\eat{
Finally, \sysname runs applications on Google Cloud Preemptible VMs and addresses the unique reliability and systems challenges arising from their maximum 24-hour lifetime set by the provider.
We present the \emph{first} empirical model and analysis of transient server availability that is \emph{not} rooted in classical bidding models for EC2 spot instances that have been proposed thus far. Our empirical model allows us to predict expected running times and costs of different scientific computing applications, which informs our bags of jobs execution policies.
}

Our empirical preemption model, cost-optimizing server-selection policies, and the bag of jobs abstraction are implemented as part of the \sysname framework, and we make the following contributions:
%\vspace*{\largesubsecspace}
\begin{enumerate}[leftmargin=12pt]
% Yeah the "not spot" point below may need clarification before contribs
%\item Since transient server preemptions can disrupt the execution of jobs, we present the \emph{first} empirical model and analysis of transient server availability that is \emph{not} rooted in classical bidding models for EC2 spot instances that have been proposed thus far. Our empirical model allows us to predict expected running times and costs of different scientific computing applications.
\item We develop a new analytical model based on a large-scale, first-of-its-kind empirical study of lifetimes of Google preemptible VMs for understanding and characterizing their preemption dynamics. Our model captures the key effects resulting from the 24 hour lifetime constraint associated with these VMs, and enables accurate prediction of expected running times and costs of different scientific computing applications.
% WTF is even partial redundancy? 
  
\item In order to select the optimal VM for an application, from the plethora of choices offered by cloud providers, we develop a transient VM selection policy that minimizes the cost of running applications. Our search based policy selects a transient VM based on it's cost, performance, and preemption rate. 



\item We implement all our policies as part of a new framework, \sysname, and evaluate the cost and performance of different representative scientific computing  applications on the Google Cloud Platform. Compared to conventional cloud deployments, \sysname can reduce costs of running bags of jobs by more than $5\times$, and when compared to dedicated HPC clusters, it can reduce the total turnaround time by up to an order of magnitude. 

%\item Finally, ease of use and extensibility are one of the ``first principles'' in the design of \sysname, and we present the design and implementation of the system components and present case studies of how  scientific applications such as molecular dynamics simulations can be easily deployed on transient cloud VMs. 
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% We need to explain the bag of jobs metaphor very clearly here!

\begin{comment}
Our policies for  mitigating transiency build on one k


\sysname~runs unmodified parallel scientific applications, and is a cost and transience aware cluster manager. 




% What about the bag of jobs!?!?!

% And where is SciSpot!?!

% There are no references to related work, this is just a nice long story so far! 

%Heterogeneity of the types of VMs and pricing models. Cost becomes important. 


%Public clouds can supplement and complement the supercomputing infrastructure. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Running parallel scientific applications, such as molecular dynamics (MD) simulations, on low-cost cloud transient resources. 

The first "big" idea is that simulations are often bag of parallel tasks. 

While there has been some past work that looks at running MPI applications on spot instances, our scope is much broader and considers how complete simulation pipelines can be run at low cost. 

Spiel on transient instances. Increasingly popular resource allocation model that is being offered by all cloud providers. 
Very low cost compared to conventional cloud resources, often by up to 10x. 
However, can be frequently revoked. 
Thus failure is a common occurrence, and not a rare-event. 
This is especially challenging for MPI jobs because of its inability to tolerate failures. 

However, our insight is that while protecting a *single* job against revocations can require elaborate checkpointing based approaches, we dont necessarily have to do that if we consider that most simulations are composed of a series of jobs that search over a parameter space, and that what is important is the total running time and cost of this entire series of jobs. 

Thus, no single job is "special". 

Another aspect of novelty is that past work on transient resources used EC2 pricing information to get failure probabilities. However, this is no longer an accurate method. We perform the first empirical study of google preemptible VMs and their performance and availability for HPC workloads. 

Another fundamental question is what is a suitable metric in such cases. Conventionally, it is speedup. In the cloud, it is some combination of cost and running time. 


\sysname is a framework and a tool that combines the use of failure modeling, checkpointing, and application-aware early stopping, to provide low cost execution of jobgroups for scientific applications.


Our work is the first to make a principled study of transient instances \emph{other} than Amazon spot instances.
Furthermore, our techniques make the first stab at addressing the new problems in the new EC2 spot pricing scheme.

Our work is in the context of reliability and cloud execution of scientific applications, and is novel because of multiple reasons:
1. Reliability and failure analysis of parallel scientific applications usually studied in the context of hardware with MTTFs of centuries, which is several orders of magnitudes higher than MTTFs faced in transient cloud servers (few hours).

2. While there have been studies of scientific applications been deployed in the context of cloud platforms, to the best of our knowledge, there has been no effort that integrates server selection and running jobgroups in a convenient automatic manner that makes it feasible to actually deploy applications on the cloud for scientists who may not have the requisite cloud experience.
\end{comment}

\begin{comment}
Notes:
  
For scientific applications, public clouds offer many advantages such as no waiting/queuing time and instant access to a wide range of resources, and pay as you go pricing. 
However, judicious use of cloud resources is necessary to achieve high performance and to avoid cost overruns.

Increasingly, cloud providers are offering transient VMs that are sold at steeply discounted rates of 90\%.
However, they can be unilaterally revoked by the cloud provider, resulting in preemptions which are akin to fail-stop failures.
This is 


The inspiration for our work is the massively parallel molecular simulation pipelines that have been proposed recently that call for a large number of simulation parameters to be evaluated.
inspiration/case-study

Bag of jobs useful in many contexts. 


Easy to use and deploy tool that does not need special access to high performance computing systems. 

Cloud will be the key piece of software infrastructure for scientific applications.
However, the resource management and orchestration principles, techniques, and tools, are all different.
So in this paper, we design and develop mechanisms and policies for running these applications at \emph{low cost}.

Software framework.
Easy to run.
Auto-tuning and requires only specifying the program to run.
Auto-tunes resource allocation to minimize cost and running time.



Designing Materials to Revolutionize and Engineer ourFuture (DMREF) : Google cloud mention

CSSI: Robust service .
Framework implementation
 


\end{comment}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:

